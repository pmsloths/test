{"config":{"indexing":"full","lang":["en","vi"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"VietNam Zone \u00b6 Database \u0111\u01a1n v\u1ecb h\u00e0nh ch\u00ednh c\u1ee7a Vi\u1ec7t Nam hhhhhhh D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u1ea5y tr\u1ef1c ti\u1ebfp t\u1eeb T\u1ed5ng C\u1ee5c Th\u1ed1ng K\u00ea Vi\u1ec7t Nam . C\u1eadp nh\u1eadt l\u1ea7n cu\u1ed1i: 11/04/2022 1. C\u00e0i \u0111\u1eb7t \u00b6 1.1 C\u00e0i \u0111\u1eb7t g\u00f3i b\u1eb1ng composer \u00b6 composer require kjmtrue/vietnam-zone 1.2 Copy file migration \u00b6 php artisan vendor:publish --provider = \"Kjmtrue\\VietnamZone\\ServiceProvider\" 1.3 Ch\u1ec9nh s\u1eeda file migration n\u1ebfu c\u1ea7n \u00b6 M\u1edf c\u00e1c file migration sau v\u00e0 tu\u1ef3 ch\u1ec9nh theo y\u00eau c\u1ea7u ri\u00eang c\u1ee7a b\u1ea1n. database/migrations/2020_01_01_000001_create_provinces_table.php database/migrations/2020_01_01_000002_create_districts_table.php database/migrations/2020_01_01_000003_create_wards_table.php 2. Ch\u1ea1y migration \u00b6 php artisan migrate 3. Import d\u1eef li\u1ec7u \u00b6 php artisan vietnamzone:import L\u01b0u \u00fd: - D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c c\u1eadp nh\u1eadt l\u1ea7n cu\u1ed1i: 11/04/2022 - \u0110\u1ec3 c\u1eadp nh\u1eadt d\u1eef li\u1ec7u m\u1edbi nh\u1ea5t, vui l\u00f2ng l\u00e0m theo h\u01b0\u1edbng d\u1eabn \u1edf m\u1ee5c 5 tr\u01b0\u1edbc khi ch\u1ea1y l\u1ec7nh php artisan vietnamzone:import 4. S\u1eed d\u1ee5ng \u00b6 $provinces = \\Kjmtrue\\VietnamZone\\Models\\Province::get(); $districts = \\Kjmtrue\\VietnamZone\\Models\\District::whereProvinceId(50)->get(); $wards = \\Kjmtrue\\VietnamZone\\Models\\Ward::whereDistrictId(552)->get(); 5. T\u1ea3i file d\u1eef li\u1ec7u \u00b6 D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb T\u1ed5ng C\u1ee5c Th\u1ed1ng K\u00ea Vi\u1ec7t Nam . Trong t\u01b0\u01a1ng lai, khi c\u01a1 quan c\u00f3 th\u1ea9m quy\u1ec1n s\u1eafp x\u1ebfp l\u1ea1i c\u00e1c \u0111\u01a1n v\u1ecb h\u00e0nh ch\u00ednh th\u00ec b\u1ea1n c\u1ea7n ph\u1ea3i t\u1ea3i file d\u1eef li\u1ec7u m\u1edbi nh\u1ea5t tr\u01b0\u1edbc khi import d\u1eef li\u1ec7u v\u00e0o d\u1ef1 \u00e1n c\u1ee7a b\u1ea1n. B\u1ea1n vui l\u00f2ng l\u00e0m theo c\u00e1c b\u01b0\u1edbc h\u01b0\u1edbng d\u1eabn d\u01b0\u1edbi \u0111\u00e2y: Truy c\u1eadp: https://danhmuchanhchinh.gso.gov.vn/ (URL n\u00e0y c\u00f3 th\u1ec3 b\u1ecb GSOVN thay \u0111\u1ed5i) T\u00ecm n\u00fat \"Xu\u1ea5t Excel\" Tick v\u00e0o \u00f4 checkbox \"Qu\u1eadn Huy\u1ec7n Ph\u01b0\u1eddng X\u00e3\" Click v\u00e0o n\u00fat \"Xu\u1ea5t Excel\", v\u00e0 t\u1ea3i file xls v\u1ec1 \u0110\u1ed5i t\u00ean file v\u1eeba t\u1ea3i v\u1ec1 th\u00e0nh vnzone.xls v\u00e0 copy v\u00e0o th\u01b0 m\u1ee5c storage c\u1ee7a d\u1ef1 \u00e1n Ch\u1ea1y l\u1ec7nh php artisan vietnamzone:import \u1edf b\u01b0\u1edbc 3 Todo \u00b6 [ ] C\u1eadp nh\u1eadt d\u1eef li\u1ec7u [ ] Download file tr\u1ef1c ti\u1ebfp t\u1eeb website t\u1ed5ng c\u1ee5c th\u1ed1ng k\u00ea Screenshot \u00b6 select * from provinces +----+------------------------+--------+---------------------+---------------------+ | id | name | gso_id | created_at | updated_at | +----+------------------------+--------+---------------------+---------------------+ | 1 | Th\u00e0nh ph\u1ed1 H\u00e0 N\u1ed9i | 01 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 2 | T\u1ec9nh H\u00e0 Giang | 02 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 3 | T\u1ec9nh Cao B\u1eb1ng | 04 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 4 | T\u1ec9nh B\u1eafc K\u1ea1n | 06 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 5 | T\u1ec9nh Tuy\u00ean Quang | 08 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | +----+------------------------+--------+---------------------+---------------------+ select * from districts +----+-------------------+--------+-------------+---------------------+---------------------+ | id | name | gso_id | province_id | created_at | updated_at | +----+-------------------+--------+-------------+---------------------+---------------------+ | 1 | Qu\u1eadn Ba \u0110\u00ecnh | 001 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 2 | Qu\u1eadn Ho\u00e0n Ki\u1ebfm | 002 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 3 | Qu\u1eadn T\u00e2y H\u1ed3 | 003 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 4 | Qu\u1eadn Long Bi\u00ean | 004 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 5 | Qu\u1eadn C\u1ea7u Gi\u1ea5y | 005 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | +----+-------------------+--------+-------------+---------------------+---------------------+ select * from wards +----+--------------------------+--------+-------------+---------------------+---------------------+ | id | name | gso_id | district_id | created_at | updated_at | +----+--------------------------+--------+-------------+---------------------+---------------------+ | 1 | Ph\u01b0\u1eddng Ph\u00fac X\u00e1 | 00001 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | | 2 | Ph\u01b0\u1eddng Tr\u00fac B\u1ea1ch | 00004 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | | 3 | Ph\u01b0\u1eddng V\u0129nh Ph\u00fac | 00006 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | | 4 | Ph\u01b0\u1eddng C\u1ed1ng V\u1ecb | 00007 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | | 5 | Ph\u01b0\u1eddng Li\u1ec5u Giai | 00008 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | +----+--------------------------+--------+-------------+---------------------+---------------------+","title":"Index"},{"location":"#vietnam-zone","text":"Database \u0111\u01a1n v\u1ecb h\u00e0nh ch\u00ednh c\u1ee7a Vi\u1ec7t Nam hhhhhhh D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u1ea5y tr\u1ef1c ti\u1ebfp t\u1eeb T\u1ed5ng C\u1ee5c Th\u1ed1ng K\u00ea Vi\u1ec7t Nam . C\u1eadp nh\u1eadt l\u1ea7n cu\u1ed1i: 11/04/2022","title":"VietNam Zone"},{"location":"#1-cai-at","text":"","title":"1. C\u00e0i \u0111\u1eb7t"},{"location":"#11-cai-at-goi-bang-composer","text":"composer require kjmtrue/vietnam-zone","title":"1.1 C\u00e0i \u0111\u1eb7t g\u00f3i b\u1eb1ng composer"},{"location":"#12-copy-file-migration","text":"php artisan vendor:publish --provider = \"Kjmtrue\\VietnamZone\\ServiceProvider\"","title":"1.2 Copy file migration"},{"location":"#13-chinh-sua-file-migration-neu-can","text":"M\u1edf c\u00e1c file migration sau v\u00e0 tu\u1ef3 ch\u1ec9nh theo y\u00eau c\u1ea7u ri\u00eang c\u1ee7a b\u1ea1n. database/migrations/2020_01_01_000001_create_provinces_table.php database/migrations/2020_01_01_000002_create_districts_table.php database/migrations/2020_01_01_000003_create_wards_table.php","title":"1.3 Ch\u1ec9nh s\u1eeda file migration n\u1ebfu c\u1ea7n"},{"location":"#2-chay-migration","text":"php artisan migrate","title":"2. Ch\u1ea1y migration"},{"location":"#3-import-du-lieu","text":"php artisan vietnamzone:import L\u01b0u \u00fd: - D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c c\u1eadp nh\u1eadt l\u1ea7n cu\u1ed1i: 11/04/2022 - \u0110\u1ec3 c\u1eadp nh\u1eadt d\u1eef li\u1ec7u m\u1edbi nh\u1ea5t, vui l\u00f2ng l\u00e0m theo h\u01b0\u1edbng d\u1eabn \u1edf m\u1ee5c 5 tr\u01b0\u1edbc khi ch\u1ea1y l\u1ec7nh php artisan vietnamzone:import","title":"3. Import d\u1eef li\u1ec7u"},{"location":"#4-su-dung","text":"$provinces = \\Kjmtrue\\VietnamZone\\Models\\Province::get(); $districts = \\Kjmtrue\\VietnamZone\\Models\\District::whereProvinceId(50)->get(); $wards = \\Kjmtrue\\VietnamZone\\Models\\Ward::whereDistrictId(552)->get();","title":"4. S\u1eed d\u1ee5ng"},{"location":"#5-tai-file-du-lieu","text":"D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb T\u1ed5ng C\u1ee5c Th\u1ed1ng K\u00ea Vi\u1ec7t Nam . Trong t\u01b0\u01a1ng lai, khi c\u01a1 quan c\u00f3 th\u1ea9m quy\u1ec1n s\u1eafp x\u1ebfp l\u1ea1i c\u00e1c \u0111\u01a1n v\u1ecb h\u00e0nh ch\u00ednh th\u00ec b\u1ea1n c\u1ea7n ph\u1ea3i t\u1ea3i file d\u1eef li\u1ec7u m\u1edbi nh\u1ea5t tr\u01b0\u1edbc khi import d\u1eef li\u1ec7u v\u00e0o d\u1ef1 \u00e1n c\u1ee7a b\u1ea1n. B\u1ea1n vui l\u00f2ng l\u00e0m theo c\u00e1c b\u01b0\u1edbc h\u01b0\u1edbng d\u1eabn d\u01b0\u1edbi \u0111\u00e2y: Truy c\u1eadp: https://danhmuchanhchinh.gso.gov.vn/ (URL n\u00e0y c\u00f3 th\u1ec3 b\u1ecb GSOVN thay \u0111\u1ed5i) T\u00ecm n\u00fat \"Xu\u1ea5t Excel\" Tick v\u00e0o \u00f4 checkbox \"Qu\u1eadn Huy\u1ec7n Ph\u01b0\u1eddng X\u00e3\" Click v\u00e0o n\u00fat \"Xu\u1ea5t Excel\", v\u00e0 t\u1ea3i file xls v\u1ec1 \u0110\u1ed5i t\u00ean file v\u1eeba t\u1ea3i v\u1ec1 th\u00e0nh vnzone.xls v\u00e0 copy v\u00e0o th\u01b0 m\u1ee5c storage c\u1ee7a d\u1ef1 \u00e1n Ch\u1ea1y l\u1ec7nh php artisan vietnamzone:import \u1edf b\u01b0\u1edbc 3","title":"5. T\u1ea3i file d\u1eef li\u1ec7u"},{"location":"#todo","text":"[ ] C\u1eadp nh\u1eadt d\u1eef li\u1ec7u [ ] Download file tr\u1ef1c ti\u1ebfp t\u1eeb website t\u1ed5ng c\u1ee5c th\u1ed1ng k\u00ea","title":"Todo"},{"location":"#screenshot","text":"select * from provinces +----+------------------------+--------+---------------------+---------------------+ | id | name | gso_id | created_at | updated_at | +----+------------------------+--------+---------------------+---------------------+ | 1 | Th\u00e0nh ph\u1ed1 H\u00e0 N\u1ed9i | 01 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 2 | T\u1ec9nh H\u00e0 Giang | 02 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 3 | T\u1ec9nh Cao B\u1eb1ng | 04 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 4 | T\u1ec9nh B\u1eafc K\u1ea1n | 06 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 5 | T\u1ec9nh Tuy\u00ean Quang | 08 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | +----+------------------------+--------+---------------------+---------------------+ select * from districts +----+-------------------+--------+-------------+---------------------+---------------------+ | id | name | gso_id | province_id | created_at | updated_at | +----+-------------------+--------+-------------+---------------------+---------------------+ | 1 | Qu\u1eadn Ba \u0110\u00ecnh | 001 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 2 | Qu\u1eadn Ho\u00e0n Ki\u1ebfm | 002 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 3 | Qu\u1eadn T\u00e2y H\u1ed3 | 003 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 4 | Qu\u1eadn Long Bi\u00ean | 004 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | | 5 | Qu\u1eadn C\u1ea7u Gi\u1ea5y | 005 | 1 | 2020 - 06 - 16 17:22:30 | 2020 - 06 - 16 17:22:30 | +----+-------------------+--------+-------------+---------------------+---------------------+ select * from wards +----+--------------------------+--------+-------------+---------------------+---------------------+ | id | name | gso_id | district_id | created_at | updated_at | +----+--------------------------+--------+-------------+---------------------+---------------------+ | 1 | Ph\u01b0\u1eddng Ph\u00fac X\u00e1 | 00001 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | | 2 | Ph\u01b0\u1eddng Tr\u00fac B\u1ea1ch | 00004 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | | 3 | Ph\u01b0\u1eddng V\u0129nh Ph\u00fac | 00006 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | | 4 | Ph\u01b0\u1eddng C\u1ed1ng V\u1ecb | 00007 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | | 5 | Ph\u01b0\u1eddng Li\u1ec5u Giai | 00008 | 1 | 2020 - 06 - 16 17:30:13 | 2020 - 06 - 16 17:30:13 | +----+--------------------------+--------+-------------+---------------------+---------------------+","title":"Screenshot"},{"location":"api/classification/","text":"from hulearn.classification import * \u00b6 ::: hulearn.classification.functionclassifier ::: hulearn.classification.interactiveclassifier","title":"Classification"},{"location":"api/classification/#from-hulearnclassification-import","text":"::: hulearn.classification.functionclassifier ::: hulearn.classification.interactiveclassifier","title":"from hulearn.classification import *"},{"location":"api/common/","text":"from hulearn.common import * \u00b6 ::: hulearn.common","title":"Common"},{"location":"api/common/#from-hulearncommon-import","text":"::: hulearn.common","title":"from hulearn.common import *"},{"location":"api/datasets/","text":"from hulearn.datasets import * \u00b6 ::: hulearn.datasets","title":"Datasets"},{"location":"api/datasets/#from-hulearndatasets-import","text":"::: hulearn.datasets","title":"from hulearn.datasets import *"},{"location":"api/interactive-charts/","text":"InteractiveCharts \u00b6 ::: hulearn.experimental.InteractiveCharts parallel_coordinates \u00b6 ::: hulearn.experimental.parallel_coordinates","title":"Charts"},{"location":"api/interactive-charts/#interactivecharts","text":"::: hulearn.experimental.InteractiveCharts","title":"InteractiveCharts"},{"location":"api/interactive-charts/#parallel_coordinates","text":"::: hulearn.experimental.parallel_coordinates","title":"parallel_coordinates"},{"location":"api/outlier/","text":"from hulearn.outlier import * \u00b6 ::: hulearn.outlier.functionoutlier ::: hulearn.outlier.interactiveoutlier","title":"Outlier"},{"location":"api/outlier/#from-hulearnoutlier-import","text":"::: hulearn.outlier.functionoutlier ::: hulearn.outlier.interactiveoutlier","title":"from hulearn.outlier import *"},{"location":"api/preprocessing/","text":"from hulearn.preprocessing import * \u00b6 ::: hulearn.preprocessing.pipetransformer ::: hulearn.preprocessing.interactivepreprocessor","title":"Preprocessing"},{"location":"api/preprocessing/#from-hulearnpreprocessing-import","text":"::: hulearn.preprocessing.pipetransformer ::: hulearn.preprocessing.interactivepreprocessor","title":"from hulearn.preprocessing import *"},{"location":"api/regression/","text":"from hulearn.regression import * \u00b6 ::: hulearn.regression.functionregressor","title":"Regression"},{"location":"api/regression/#from-hulearnregression-import","text":"::: hulearn.regression.functionregressor","title":"from hulearn.regression import *"},{"location":"api/rulers/","text":"CaseWhenRuler \u00b6 ::: hulearn.experimental.CaseWhenRuler","title":"Rulers"},{"location":"api/rulers/#casewhenruler","text":"::: hulearn.experimental.CaseWhenRuler","title":"CaseWhenRuler"},{"location":"examples/faq/","text":"Frequently Asked Questions \u00b6 Feel free to ask questions here . What are the Lessons Learned \u00b6 If you're interested in some of the lessons the creators of this tool learned while creating it, all you need to do is follow the python tradition. from hulearn import this Why Make This? \u00b6 Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground. But we've reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This made us wonder if we could make machine learning listen more to common sense. There's a lot of things that could go wrong otherwise. If you're interested in examples we might recommend this pydata talk . I'm getting a PORT error! \u00b6 You might get an error that looks like; ERROR : bokeh . server . views . ws : Refusing websocket connection from Origin 'http://localhost:8889' ; use -- allow - websocket - origin = localhost : 8889 or set BOKEH_ALLOW_WS_ORIGIN = localhost : 8889 to permit this ; currently we allow origins { 'localhost:8888' } This is related to something bokeh cannot do without explicit permission from jupyter. It can't be fixed by this library but you can circumvent this error by running jupyter via; python -m jupyter lab --port 8889 --allow-websocket-origin=localhost:8889 You can also set an environment variable BOKEH_ALLOW_WS_ORIGIN=localhost:8889 .","title":"FAQ"},{"location":"examples/faq/#frequently-asked-questions","text":"Feel free to ask questions here .","title":"Frequently Asked Questions"},{"location":"examples/faq/#what-are-the-lessons-learned","text":"If you're interested in some of the lessons the creators of this tool learned while creating it, all you need to do is follow the python tradition. from hulearn import this","title":"What are the Lessons Learned"},{"location":"examples/faq/#why-make-this","text":"Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground. But we've reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This made us wonder if we could make machine learning listen more to common sense. There's a lot of things that could go wrong otherwise. If you're interested in examples we might recommend this pydata talk .","title":"Why Make This?"},{"location":"examples/faq/#im-getting-a-port-error","text":"You might get an error that looks like; ERROR : bokeh . server . views . ws : Refusing websocket connection from Origin 'http://localhost:8889' ; use -- allow - websocket - origin = localhost : 8889 or set BOKEH_ALLOW_WS_ORIGIN = localhost : 8889 to permit this ; currently we allow origins { 'localhost:8888' } This is related to something bokeh cannot do without explicit permission from jupyter. It can't be fixed by this library but you can circumvent this error by running jupyter via; python -m jupyter lab --port 8889 --allow-websocket-origin=localhost:8889 You can also set an environment variable BOKEH_ALLOW_WS_ORIGIN=localhost:8889 .","title":"I'm getting a PORT error!"},{"location":"examples/model-mining/","text":"In this example, we will demonstrate that you can use visual data mining techniques to discover meaningful patterns in your data. These patterns can be easily translated into a machine learning model by using the tools found in this package. You can find a full tutorial of this technique on calmcode but the main video can be viewed below. The Task \u00b6 We're going to make a rule based model for the creditcard dataset. The main feature of the dataset is that it is suffering from a class imbalance. Instead of training a machine learning model, let's try to instead explore it with a parallel coordinates chart. If you scroll all the way to the bottom of this tutorial you'll see an example of such a chart. It shows a \"train\"-set. We explored the data just like in the video and that led us to define the following model. from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , age = 15 ): ruler = CaseWhenRuler ( default = 0 ) ( ruler . add_rule ( lambda d : ( d [ 'V11' ] > 4 ), 1 ) . add_rule ( lambda d : ( d [ 'V17' ] < - 3 ), 1 ) . add_rule ( lambda d : ( d [ 'V14' ] < - 8 ), 1 )) return ruler . predict ( dataf ) clf = FunctionClassifier ( make_prediction ) Full Code First we load the data. ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split df_credit = fetch_openml( data_id=1597, as_frame=True ) credit_train, credit_test = train_test_split(df_credit, test_size=0.5, shuffle=True) ``` Next, we create a hiplot in jupyter. ```python import json import hiplot as hip samples = [credit_train.loc[lambda d: d['group'] == True], credit_train.sample(5000)] json_data = pd.concat(samples).to_json(orient='records') hip.Experiment.from_iterable(json.loads(json_data)).display() ``` Given that we have our model, we can make a classification report. ```python from sklearn.metrics import classification_report Note that fit is a no-op here. \u00b6 preds = clf.fit(credit_train, credit_train['group']).predict(credit_test)) print(classification_report(credit_test['group'], preds) ``` When we ran the benchmark locally, we got the following classification report. precision recall f1-score support False 1.00 1.00 1.00 142165 True 0.70 0.73 0.71 239 accuracy 1.00 142404 macro avg 0.85 0.86 0.86 142404 weighted avg 1.00 1.00 1.00 142404 Deep Learning \u00b6 It's not a perfect benchmark, but we could compare this result to the one that's demonstrated on the keras blog . The trained model there lists 86.67% precision but only 23.9% recall. Depending on your preferences for false-positives, you could argue that our model is outperforming the deep learning model. It's not 100% a fair comparison. You can imagine that the keras blogpost is written to explain keras. The auther likely didn't attempt to make a state-of-the-art model. But what this demo does show is the merit of turning an exploratory data analysis into a model. You can end up with a very interpretable model, you might learn something about your data along the way and the model might certainly still perform well. Parallel Coordinates \u00b6 If you hover of the group name and right-click, you'll be able to set it for coloring and repeat the experience in the video. By doing that it becomes quite easy to eyeball how to separate the two classes. The V17 column especially seems powerful here. In real life we might ask \"why?\" this column is so distinctive but for now we'll just play around until we find a sensible model.","title":"Model Mining"},{"location":"examples/model-mining/#the-task","text":"We're going to make a rule based model for the creditcard dataset. The main feature of the dataset is that it is suffering from a class imbalance. Instead of training a machine learning model, let's try to instead explore it with a parallel coordinates chart. If you scroll all the way to the bottom of this tutorial you'll see an example of such a chart. It shows a \"train\"-set. We explored the data just like in the video and that led us to define the following model. from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , age = 15 ): ruler = CaseWhenRuler ( default = 0 ) ( ruler . add_rule ( lambda d : ( d [ 'V11' ] > 4 ), 1 ) . add_rule ( lambda d : ( d [ 'V17' ] < - 3 ), 1 ) . add_rule ( lambda d : ( d [ 'V14' ] < - 8 ), 1 )) return ruler . predict ( dataf ) clf = FunctionClassifier ( make_prediction ) Full Code First we load the data. ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split df_credit = fetch_openml( data_id=1597, as_frame=True ) credit_train, credit_test = train_test_split(df_credit, test_size=0.5, shuffle=True) ``` Next, we create a hiplot in jupyter. ```python import json import hiplot as hip samples = [credit_train.loc[lambda d: d['group'] == True], credit_train.sample(5000)] json_data = pd.concat(samples).to_json(orient='records') hip.Experiment.from_iterable(json.loads(json_data)).display() ``` Given that we have our model, we can make a classification report. ```python from sklearn.metrics import classification_report","title":"The Task"},{"location":"examples/model-mining/#note-that-fit-is-a-no-op-here","text":"preds = clf.fit(credit_train, credit_train['group']).predict(credit_test)) print(classification_report(credit_test['group'], preds) ``` When we ran the benchmark locally, we got the following classification report. precision recall f1-score support False 1.00 1.00 1.00 142165 True 0.70 0.73 0.71 239 accuracy 1.00 142404 macro avg 0.85 0.86 0.86 142404 weighted avg 1.00 1.00 1.00 142404","title":"Note that fit is a no-op here."},{"location":"examples/model-mining/#deep-learning","text":"It's not a perfect benchmark, but we could compare this result to the one that's demonstrated on the keras blog . The trained model there lists 86.67% precision but only 23.9% recall. Depending on your preferences for false-positives, you could argue that our model is outperforming the deep learning model. It's not 100% a fair comparison. You can imagine that the keras blogpost is written to explain keras. The auther likely didn't attempt to make a state-of-the-art model. But what this demo does show is the merit of turning an exploratory data analysis into a model. You can end up with a very interpretable model, you might learn something about your data along the way and the model might certainly still perform well.","title":"Deep Learning"},{"location":"examples/model-mining/#parallel-coordinates","text":"If you hover of the group name and right-click, you'll be able to set it for coloring and repeat the experience in the video. By doing that it becomes quite easy to eyeball how to separate the two classes. The V17 column especially seems powerful here. In real life we might ask \"why?\" this column is so distinctive but for now we'll just play around until we find a sensible model.","title":"Parallel Coordinates"},{"location":"examples/usage/","text":"This page contains a list of short examples that demonstrate the utility of the tools in this package. The goal for each example is to be small and consise. Precision and Subgroups \u00b6 It can be the case that for a subgroup of the population you do not need a model. Suppose that we have a session log dataset from \"World of Warcraft\". We know when people logged in, if they were part of a guild and when they stopped playing. You can create a machine learning model to predict which players are at risk of quitting the game but you might also be able to come up with some simple rules. Here is one rule that might work out swell: \"If any player was playing the video game at 24:00 on new-years eve, odds are that this person is very invested in the game and won't stop playing.\" This one rule will not cover the entire population but for the subgroup it can be an effective rule. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier classifier = SomeScikitLearnModel () def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # Override model prediction if a user is a heavy_user, no matter what res = np . where ( dataf [ 'heavy_user' ], \"stays\" , res ) return res fallback_model = FunctionClassifier ( make_decision ) No Data No Problem \u00b6 Let's say that we're interested in detecting fraud at a tax office. Even without looking at the data we can already come up with some sensible rules. Any minor making over the median income is \"suspicious\". Any person who started more than 2 companies in a year is \"suspicious\". Any person who has more than 10 bank accounts is \"suspicious\". The thing with these rules is that they are easy to explain but they are not based on data at all. In fact, they may not occur in the data at all. This means that a machine learning model may not have picked up this pattern that we're interested in. Thankfully, the lack in data can be compensated with business rules. Comfort Zone \u00b6 Models typically have a \"comfort zone\". If a new data point comes in that is very different from what the models saw before it should not be treated the same way. You can also argue that points with low proba score should also not be automated. If you want to prevent predictions where the model is \"unsure\" then you might want to follow this diagram; You can construct such a system by creating a FunctionClassifier that handles the logic you require. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier # We're importing a classifier/outlier detector from our library # but nothing is stopping you from using those in scikit-learn. # Just make sure that they are trained beforehand! outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision ) For more information on why this tactic is helpful: blogpost pydata talk","title":"S\u1eed d\u1ee5ng"},{"location":"examples/usage/#precision-and-subgroups","text":"It can be the case that for a subgroup of the population you do not need a model. Suppose that we have a session log dataset from \"World of Warcraft\". We know when people logged in, if they were part of a guild and when they stopped playing. You can create a machine learning model to predict which players are at risk of quitting the game but you might also be able to come up with some simple rules. Here is one rule that might work out swell: \"If any player was playing the video game at 24:00 on new-years eve, odds are that this person is very invested in the game and won't stop playing.\" This one rule will not cover the entire population but for the subgroup it can be an effective rule. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier classifier = SomeScikitLearnModel () def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # Override model prediction if a user is a heavy_user, no matter what res = np . where ( dataf [ 'heavy_user' ], \"stays\" , res ) return res fallback_model = FunctionClassifier ( make_decision )","title":"Precision and Subgroups"},{"location":"examples/usage/#no-data-no-problem","text":"Let's say that we're interested in detecting fraud at a tax office. Even without looking at the data we can already come up with some sensible rules. Any minor making over the median income is \"suspicious\". Any person who started more than 2 companies in a year is \"suspicious\". Any person who has more than 10 bank accounts is \"suspicious\". The thing with these rules is that they are easy to explain but they are not based on data at all. In fact, they may not occur in the data at all. This means that a machine learning model may not have picked up this pattern that we're interested in. Thankfully, the lack in data can be compensated with business rules.","title":"No Data No Problem"},{"location":"examples/usage/#comfort-zone","text":"Models typically have a \"comfort zone\". If a new data point comes in that is very different from what the models saw before it should not be treated the same way. You can also argue that points with low proba score should also not be automated. If you want to prevent predictions where the model is \"unsure\" then you might want to follow this diagram; You can construct such a system by creating a FunctionClassifier that handles the logic you require. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier # We're importing a classifier/outlier detector from our library # but nothing is stopping you from using those in scikit-learn. # Just make sure that they are trained beforehand! outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision ) For more information on why this tactic is helpful: blogpost pydata talk","title":"Comfort Zone"},{"location":"guide/drawing-classifier/drawing%20copy/","text":"Drawing as a Model \u00b6 Classic Classification Problem \u00b6 Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier. Let's Draw! \u00b6 Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas. Instructions \u00b6 To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model? Properties of Modelling Technique \u00b6 Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model. From Jupyter \u00b6 In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this; Serialize \u00b6 You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made. Model \u00b6 This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API. Conclusion \u00b6 The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems. Notebook \u00b6 If you want to run this code yourself, feel free to download the notebook .","title":"Drawing as a Model"},{"location":"guide/drawing-classifier/drawing%20copy/#drawing-as-a-model","text":"","title":"Drawing as a Model"},{"location":"guide/drawing-classifier/drawing%20copy/#classic-classification-problem","text":"Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier.","title":"Classic Classification Problem"},{"location":"guide/drawing-classifier/drawing%20copy/#lets-draw","text":"Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas.","title":"Let's Draw!"},{"location":"guide/drawing-classifier/drawing%20copy/#instructions","text":"To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model?","title":"Instructions"},{"location":"guide/drawing-classifier/drawing%20copy/#properties-of-modelling-technique","text":"Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model.","title":"Properties of Modelling Technique"},{"location":"guide/drawing-classifier/drawing%20copy/#from-jupyter","text":"In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this;","title":"From Jupyter"},{"location":"guide/drawing-classifier/drawing%20copy/#serialize","text":"You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made.","title":"Serialize"},{"location":"guide/drawing-classifier/drawing%20copy/#model","text":"This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API.","title":"Model"},{"location":"guide/drawing-classifier/drawing%20copy/#conclusion","text":"The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems.","title":"Conclusion"},{"location":"guide/drawing-classifier/drawing%20copy/#notebook","text":"If you want to run this code yourself, feel free to download the notebook .","title":"Notebook"},{"location":"guide/drawing-classifier/drawing/","text":"Drawing as a Model \u00b6 Classic Classification Problem \u00b6 Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier. Let's Draw! \u00b6 Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas. Instructions \u00b6 To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model? Properties of Modelling Technique \u00b6 Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model. From Jupyter \u00b6 In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this; Serialize \u00b6 You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made. Model \u00b6 This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API. Conclusion \u00b6 The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems. Notebook \u00b6 If you want to run this code yourself, feel free to download the notebook .","title":"Drawing as a Model VN"},{"location":"guide/drawing-classifier/drawing/#drawing-as-a-model","text":"","title":"Drawing as a Model"},{"location":"guide/drawing-classifier/drawing/#classic-classification-problem","text":"Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier.","title":"Classic Classification Problem"},{"location":"guide/drawing-classifier/drawing/#lets-draw","text":"Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas.","title":"Let's Draw!"},{"location":"guide/drawing-classifier/drawing/#instructions","text":"To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model?","title":"Instructions"},{"location":"guide/drawing-classifier/drawing/#properties-of-modelling-technique","text":"Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model.","title":"Properties of Modelling Technique"},{"location":"guide/drawing-classifier/drawing/#from-jupyter","text":"In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this;","title":"From Jupyter"},{"location":"guide/drawing-classifier/drawing/#serialize","text":"You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made.","title":"Serialize"},{"location":"guide/drawing-classifier/drawing/#model","text":"This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API.","title":"Model"},{"location":"guide/drawing-classifier/drawing/#conclusion","text":"The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems.","title":"Conclusion"},{"location":"guide/drawing-classifier/drawing/#notebook","text":"If you want to run this code yourself, feel free to download the notebook .","title":"Notebook"},{"location":"guide/drawing-features/custom-features/","text":"Sofar we've explored drawing as a tool for models, but it can also be used as a tool to generate features. To explore this, let's load in the penguins dataset again. from sklego.datasets import load_penguins test df = load_penguins ( as_frame = True ) . dropna () Drawing \u00b6 We can draw over this dataset. It's like before but with one crucial differenc from hulearn.experimental.interactive import InteractiveCharts # Note that the `labels` arugment here is a list, not a string! This # tells the tool that we want to be able to add custom groups that are # not defined by a column in the dataframe. charts = InteractiveCharts ( df , labels = [ 'group_one' , 'group_two' ]) Let's make a custom drawing. charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) Let's assume the new drawing looks something like this. Sofar these drawn features have been used to construct models. But they can also be used to help label data or generate extra features for machine learning models. Features \u00b6 This library makes it easy to add these features to scikit-learn pipelines or to pandas. To get started, you'll want to import the InteractivePreprocessor . from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor ( json_desc = charts . data ()) This tfm object is can be used as a preprocessing step inside of scikit-learn but it can also be used in a pandas pipeline. # The flow for scikit-learn tfm . fit ( df ) . transform ( df ) # The flow for pandas df . pipe ( tfm . pandas_pipe )","title":"Drawing Features VN"},{"location":"guide/drawing-features/custom-features/#drawing","text":"We can draw over this dataset. It's like before but with one crucial differenc from hulearn.experimental.interactive import InteractiveCharts # Note that the `labels` arugment here is a list, not a string! This # tells the tool that we want to be able to add custom groups that are # not defined by a column in the dataframe. charts = InteractiveCharts ( df , labels = [ 'group_one' , 'group_two' ]) Let's make a custom drawing. charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) Let's assume the new drawing looks something like this. Sofar these drawn features have been used to construct models. But they can also be used to help label data or generate extra features for machine learning models.","title":"Drawing"},{"location":"guide/drawing-features/custom-features/#features","text":"This library makes it easy to add these features to scikit-learn pipelines or to pandas. To get started, you'll want to import the InteractivePreprocessor . from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor ( json_desc = charts . data ()) This tfm object is can be used as a preprocessing step inside of scikit-learn but it can also be used in a pandas pipeline. # The flow for scikit-learn tfm . fit ( df ) . transform ( df ) # The flow for pandas df . pipe ( tfm . pandas_pipe )","title":"Features"},{"location":"guide/finding-outliers/outliers/","text":"Rethinking Classification \u00b6 Let's have another look at the interactive canvas that we saw in the previous guide. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The drawn colors indicate that a human deemed a classification appropriate. You could wonder what we might want to do with the regions that have not been colored though. Machine learning algorithms might typically still assign a class to those regions but that can be a dangerous idea. We might consider these points outside of the \"comfort zone\" of the predicted areas. In these situations it might be best to declare it an outlier and to handle it differently. That way we don't automate a decision that we're likely to regret later. Outliers \u00b6 The drawn charts can be used to construct a classifier but they may also be used to construct an outlier detection model. This allows us to re-use earlier work for multiple purposes. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () charts = InteractiveCharts ( df , labels = \"species\" ) # Run this in a seperate cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Run this in a seperate cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) To demonstrate how it works, let's assume that we've drawn the following: We'll again fetch the drawn data but now we'll use it to detect outliers. from hulearn.outlier import InteractiveOutlierDetector # Load the model using drawn-data. model = InteractiveOutlierDetector ( json_desc = charts . data ()) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict ( X ) This model can now be used as a scikit-learn compatible outlier detection model. Here's the output of the model. Code for the plots. ```python import matplotlib.pylab as plt plt.figure(figsize=(10, 4)) plt.subplot(121) plt.scatter(X['bill_length_mm'], X['bill_depth_mm'], c=preds) plt.xlabel('bill_length_mm') plt.ylabel('bill_depth_mm') plt.subplot(122) plt.scatter(X['flipper_length_mm'], X['body_mass_g'], c=preds) plt.xlabel('flipper_length_mm') plt.ylabel('body_mass_g'); ``` How it works. \u00b6 A point is considered an outlier if it does not fall inside of enough drawn polygons. The number of poylgons that a point must fall into is a parameter that you can set manually or even search for in a grid-search. For example, let's repeat the exercise. The base setting is that a point needs to be in at least one polygon but we can change this to two. # Before model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 1 ) # After model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 2 ) Combine \u00b6 You might wonder, can we combine the FunctionClassifier with an outlier model like we've got here? Yes! Use a FunctionClassifier ! As an illustrative example we'll implement a diagram like above as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision )","title":"Outliers and Comfort VN"},{"location":"guide/finding-outliers/outliers/#rethinking-classification","text":"Let's have another look at the interactive canvas that we saw in the previous guide. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The drawn colors indicate that a human deemed a classification appropriate. You could wonder what we might want to do with the regions that have not been colored though. Machine learning algorithms might typically still assign a class to those regions but that can be a dangerous idea. We might consider these points outside of the \"comfort zone\" of the predicted areas. In these situations it might be best to declare it an outlier and to handle it differently. That way we don't automate a decision that we're likely to regret later.","title":"Rethinking Classification"},{"location":"guide/finding-outliers/outliers/#outliers","text":"The drawn charts can be used to construct a classifier but they may also be used to construct an outlier detection model. This allows us to re-use earlier work for multiple purposes. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () charts = InteractiveCharts ( df , labels = \"species\" ) # Run this in a seperate cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Run this in a seperate cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) To demonstrate how it works, let's assume that we've drawn the following: We'll again fetch the drawn data but now we'll use it to detect outliers. from hulearn.outlier import InteractiveOutlierDetector # Load the model using drawn-data. model = InteractiveOutlierDetector ( json_desc = charts . data ()) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict ( X ) This model can now be used as a scikit-learn compatible outlier detection model. Here's the output of the model. Code for the plots. ```python import matplotlib.pylab as plt plt.figure(figsize=(10, 4)) plt.subplot(121) plt.scatter(X['bill_length_mm'], X['bill_depth_mm'], c=preds) plt.xlabel('bill_length_mm') plt.ylabel('bill_depth_mm') plt.subplot(122) plt.scatter(X['flipper_length_mm'], X['body_mass_g'], c=preds) plt.xlabel('flipper_length_mm') plt.ylabel('body_mass_g'); ```","title":"Outliers"},{"location":"guide/finding-outliers/outliers/#how-it-works","text":"A point is considered an outlier if it does not fall inside of enough drawn polygons. The number of poylgons that a point must fall into is a parameter that you can set manually or even search for in a grid-search. For example, let's repeat the exercise. The base setting is that a point needs to be in at least one polygon but we can change this to two. # Before model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 1 ) # After model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 2 )","title":"How it works."},{"location":"guide/finding-outliers/outliers/#combine","text":"You might wonder, can we combine the FunctionClassifier with an outlier model like we've got here? Yes! Use a FunctionClassifier ! As an illustrative example we'll implement a diagram like above as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision )","title":"Combine"},{"location":"guide/function-classifier/function-classifier/","text":"The goal of this library is to make it easier to declare common sense models. A very pythonic way of getting there is to declare a function. One of the first features in this library is the ability to re-use functions as if they are scikit-learn models. Titanic \u00b6 Let's see how this might work. We'll grab a dataset that is packaged along with this library. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) df . head () The df variable represents a dataframe and it has the following contents: survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe. Preparation \u00b6 To prepare our data we will first get it into the common X , y format for scikit-learn. X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense? It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better. FunctionClassifier \u00b6 Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the FunctionClassifier . You can see an example of that below. import numpy as np from hulearn.classification import FunctionClassifier def fare_based ( dataf , threshold = 10 ): \"\"\" The assumption is that folks who paid more are wealthier and are more likely to have recieved access to lifeboats. \"\"\" return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) mod = FunctionClassifier ( fare_based ) This mod is a scikit-learn model, which means that you can .fit(X, y).predict(X) . mod . fit ( X , y ) . predict ( X ) During the .fit(X, y) -step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step. Grid \u00b6 Being able to .fit(X, y).predict(X) is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best threshold value? For that, you might like to use GridSearchCV . from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score , recall_score , accuracy_score , make_scorer # Note the threshold keyword argument in this function. def fare_based ( dataf , threshold = 10 ): return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) # Pay attention here, we set the threshold argument in here. mod = FunctionClassifier ( fare_based , threshold = 10 ) # The GridSearch object can now \"grid-search\" over this argument. # We also add a bunch of metrics to our approach so we can measure. grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'threshold' : np . linspace ( 0 , 100 , 30 )}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) If we make a chart of the grid.cv_results_ then they would look something like; A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a RandomForestClassifier using the 'pclass', 'sex', 'age', 'fare' columns that the precision score would be about the same. Bigger Grids \u00b6 You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that. def last_name ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) # Once again, remember to declare your arguments here too! mod = FunctionClassifier ( last_name , pclass = 10 , sex = 'male' ) # The arguments of the function can now be \"grid-searched\". grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) Guidance \u00b6 Human Learn doesn't just allow you to turn functions into classifiers. It also tries to help you find rules that could be useful. In particular, an interactive parallel coordinates chart could be very helpful here. You can create a parallel coordinates chart directly inside of jupyter. from hulearn.experimental.interactive import parallel_coordinates parallel_coordinates ( df , label = \"survived\" , height = 200 ) What follows next are some explorations of the dataset. They are based on the scene from the titanic movie where they yell \"Woman and Children First!\". So let's see if we can confirm if this holds true. Explore \u00b6 It indeed seems that women in 1st/2nd class have a high chance of surviving. It also seems that male children have an increased change of survival, but only if they were travelling 1st/2nd class. Grid \u00b6 Here's a lovely observation. By doing exploratory analysis we not only understand the data better but we can now also turn the patterns that we've observed into a model! def make_prediction ( dataf , age = 15 ): women_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'sex' ] == \"female\" ) children_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'age' ] <= age ) return women_rule | children_rule mod = FunctionClassifier ( make_prediction ) We're even able to use grid-search again to find the optimal threshold for \"age\" . Comparison \u00b6 To compare our results we've also trained a RandomForestClassifier . Here's how the models compare; Model accuracy precision recall Women & Children Rule 0.808157 0.952168 0.558621 RandomForestClassifier 0.813869 0.785059 0.751724 It seems like our rule based model is quite reasonable. A great follow-up exercise would be to try and understand when the random forest model disagrees with the rule based system. This could lead us to understand more patterns in the data. Conclusion \u00b6 In this guide we've seen the FunctionClassifier in action. It is one of the many models in this library that will help you construct more \"human\" models. This component is very effective when it is combined with exploratory data analysis techniques. Notebook \u00b6 If you want to download with this code yourself, feel free to download the notebook here .","title":"Function as a Model VN"},{"location":"guide/function-classifier/function-classifier/#titanic","text":"Let's see how this might work. We'll grab a dataset that is packaged along with this library. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) df . head () The df variable represents a dataframe and it has the following contents: survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe.","title":"Titanic"},{"location":"guide/function-classifier/function-classifier/#preparation","text":"To prepare our data we will first get it into the common X , y format for scikit-learn. X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense? It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better.","title":"Preparation"},{"location":"guide/function-classifier/function-classifier/#functionclassifier","text":"Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the FunctionClassifier . You can see an example of that below. import numpy as np from hulearn.classification import FunctionClassifier def fare_based ( dataf , threshold = 10 ): \"\"\" The assumption is that folks who paid more are wealthier and are more likely to have recieved access to lifeboats. \"\"\" return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) mod = FunctionClassifier ( fare_based ) This mod is a scikit-learn model, which means that you can .fit(X, y).predict(X) . mod . fit ( X , y ) . predict ( X ) During the .fit(X, y) -step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step.","title":"FunctionClassifier"},{"location":"guide/function-classifier/function-classifier/#grid","text":"Being able to .fit(X, y).predict(X) is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best threshold value? For that, you might like to use GridSearchCV . from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score , recall_score , accuracy_score , make_scorer # Note the threshold keyword argument in this function. def fare_based ( dataf , threshold = 10 ): return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) # Pay attention here, we set the threshold argument in here. mod = FunctionClassifier ( fare_based , threshold = 10 ) # The GridSearch object can now \"grid-search\" over this argument. # We also add a bunch of metrics to our approach so we can measure. grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'threshold' : np . linspace ( 0 , 100 , 30 )}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) If we make a chart of the grid.cv_results_ then they would look something like; A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a RandomForestClassifier using the 'pclass', 'sex', 'age', 'fare' columns that the precision score would be about the same.","title":"Grid"},{"location":"guide/function-classifier/function-classifier/#bigger-grids","text":"You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that. def last_name ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) # Once again, remember to declare your arguments here too! mod = FunctionClassifier ( last_name , pclass = 10 , sex = 'male' ) # The arguments of the function can now be \"grid-searched\". grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y )","title":"Bigger Grids"},{"location":"guide/function-classifier/function-classifier/#guidance","text":"Human Learn doesn't just allow you to turn functions into classifiers. It also tries to help you find rules that could be useful. In particular, an interactive parallel coordinates chart could be very helpful here. You can create a parallel coordinates chart directly inside of jupyter. from hulearn.experimental.interactive import parallel_coordinates parallel_coordinates ( df , label = \"survived\" , height = 200 ) What follows next are some explorations of the dataset. They are based on the scene from the titanic movie where they yell \"Woman and Children First!\". So let's see if we can confirm if this holds true.","title":"Guidance"},{"location":"guide/function-classifier/function-classifier/#explore","text":"It indeed seems that women in 1st/2nd class have a high chance of surviving. It also seems that male children have an increased change of survival, but only if they were travelling 1st/2nd class.","title":"Explore"},{"location":"guide/function-classifier/function-classifier/#grid_1","text":"Here's a lovely observation. By doing exploratory analysis we not only understand the data better but we can now also turn the patterns that we've observed into a model! def make_prediction ( dataf , age = 15 ): women_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'sex' ] == \"female\" ) children_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'age' ] <= age ) return women_rule | children_rule mod = FunctionClassifier ( make_prediction ) We're even able to use grid-search again to find the optimal threshold for \"age\" .","title":"Grid"},{"location":"guide/function-classifier/function-classifier/#comparison","text":"To compare our results we've also trained a RandomForestClassifier . Here's how the models compare; Model accuracy precision recall Women & Children Rule 0.808157 0.952168 0.558621 RandomForestClassifier 0.813869 0.785059 0.751724 It seems like our rule based model is quite reasonable. A great follow-up exercise would be to try and understand when the random forest model disagrees with the rule based system. This could lead us to understand more patterns in the data.","title":"Comparison"},{"location":"guide/function-classifier/function-classifier/#conclusion","text":"In this guide we've seen the FunctionClassifier in action. It is one of the many models in this library that will help you construct more \"human\" models. This component is very effective when it is combined with exploratory data analysis techniques.","title":"Conclusion"},{"location":"guide/function-classifier/function-classifier/#notebook","text":"If you want to download with this code yourself, feel free to download the notebook here .","title":"Notebook"},{"location":"guide/function-preprocess/function-preprocessing/","text":"In python the most popular data analysis tool is pandas while the most popular tool for making models is scikit-learn. We love the data wrangling tools of pandas while we appreciate the benchmarking capability of scikit-learn. The fact that these tools don't fully interact is slightly awkward. The data going into the model has an big effect on the output. So how might we more easily combine the two? Pipe \u00b6 In pandas there's an amazing trick that you can do with the .pipe method. We'll give a quick overview on how it works but if you're new to this idea you may appreciate this resource or this blogpost . from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] X . head ( 4 ) The goal of the titanic dataset is to predict weather or not a passenger survived the disaster. The X variable represents a dataframe with variables that we're going to use to predict survival (stored in y ). Here's a preview of what X might have. pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code. X [ 'nchar' ] = X [ 'name' ] . str . len () This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function. def process ( dataf ): # Make a copy of the dataframe to prevent it from overwriting the original data. dataf = dataf . copy () # Make the changes dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () # Return the name dataframe return dataf We now have a nice function that makes our changes and we can use it like so; X_new = process ( X ) We can do something more powerful though. Paramaters \u00b6 Let's make some more changes to our process function. def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end. We've changed the way we've defined our function but we're also changing the way that we're going to apply it. # This is equivalent to preprocessing(X) X . pipe ( preprocessing ) The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview. With .pipe() \u00b6 ( df . pipe ( set_col_types ) . pipe ( preprocessing , nchar = True , gender = False ) . pipe ( add_time_info )) Without .pipe() \u00b6 add_time_info ( preprocessing ( set_col_types ( df ), nchar = True , gender = False )) Let's be honest, this looks messy. PipeTransformer \u00b6 It would be great if we could use the preprocessing -function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas .pipe -line in general! For that we've got another feature in our library, the PipeTransformer . from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) # Important, don't forget to declare `n_char` and `gender` here. tfm = PipeTransformer ( preprocessing , n_char = True , gender = True )) The tfm variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function. from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV pipe = Pipeline ([ ( 'prep' , tfm ), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) Once trained we can fetch the grid.cv_results_ to get a glimpse at the results of our pipeline. param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 It seems that we gender of the passenger has more of an effect on their survival than the length of their name. Utility \u00b6 The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code. There's a few small caveats to be aware of. Don't remove data \u00b6 Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this. Don't sort data \u00b6 You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the y variable that you're trying to predict. Don't use lambda \u00b6 There's two ways that you can add a new column to pandas. # Method 1 dataf_new = dataf . copy () # Don't overwrite data! dataf_new [ 'new_column' ] = dataf_new [ 'old_column' ] * 2 # Method 2 dataf_new = dataf . assign ( lambda d : d [ 'old_column' ] * 2 ) In many cases you might argue that method #2 is safer because you do not need to worry about the dataf.copy() that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use lambda functions because it cannot pickle the code. Don't Cheat! \u00b6 The functions that you write are supposed to be stateless in the sense that they don't learn from the data that goes in. You could theoretically bypass this with global variables but by doing so you're doing yourself a disservice. If you do this you'll be cheating the statistics by leaking information.","title":"Human Preprocessing VN"},{"location":"guide/function-preprocess/function-preprocessing/#pipe","text":"In pandas there's an amazing trick that you can do with the .pipe method. We'll give a quick overview on how it works but if you're new to this idea you may appreciate this resource or this blogpost . from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] X . head ( 4 ) The goal of the titanic dataset is to predict weather or not a passenger survived the disaster. The X variable represents a dataframe with variables that we're going to use to predict survival (stored in y ). Here's a preview of what X might have. pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code. X [ 'nchar' ] = X [ 'name' ] . str . len () This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function. def process ( dataf ): # Make a copy of the dataframe to prevent it from overwriting the original data. dataf = dataf . copy () # Make the changes dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () # Return the name dataframe return dataf We now have a nice function that makes our changes and we can use it like so; X_new = process ( X ) We can do something more powerful though.","title":"Pipe"},{"location":"guide/function-preprocess/function-preprocessing/#paramaters","text":"Let's make some more changes to our process function. def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end. We've changed the way we've defined our function but we're also changing the way that we're going to apply it. # This is equivalent to preprocessing(X) X . pipe ( preprocessing ) The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview.","title":"Paramaters"},{"location":"guide/function-preprocess/function-preprocessing/#with-pipe","text":"( df . pipe ( set_col_types ) . pipe ( preprocessing , nchar = True , gender = False ) . pipe ( add_time_info ))","title":"With .pipe()"},{"location":"guide/function-preprocess/function-preprocessing/#without-pipe","text":"add_time_info ( preprocessing ( set_col_types ( df ), nchar = True , gender = False )) Let's be honest, this looks messy.","title":"Without .pipe()"},{"location":"guide/function-preprocess/function-preprocessing/#pipetransformer","text":"It would be great if we could use the preprocessing -function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas .pipe -line in general! For that we've got another feature in our library, the PipeTransformer . from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) # Important, don't forget to declare `n_char` and `gender` here. tfm = PipeTransformer ( preprocessing , n_char = True , gender = True )) The tfm variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function. from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV pipe = Pipeline ([ ( 'prep' , tfm ), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) Once trained we can fetch the grid.cv_results_ to get a glimpse at the results of our pipeline. param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 It seems that we gender of the passenger has more of an effect on their survival than the length of their name.","title":"PipeTransformer"},{"location":"guide/function-preprocess/function-preprocessing/#utility","text":"The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code. There's a few small caveats to be aware of.","title":"Utility"},{"location":"guide/function-preprocess/function-preprocessing/#dont-remove-data","text":"Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this.","title":"Don't remove data"},{"location":"guide/function-preprocess/function-preprocessing/#dont-sort-data","text":"You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the y variable that you're trying to predict.","title":"Don't sort data"},{"location":"guide/function-preprocess/function-preprocessing/#dont-use-lambda","text":"There's two ways that you can add a new column to pandas. # Method 1 dataf_new = dataf . copy () # Don't overwrite data! dataf_new [ 'new_column' ] = dataf_new [ 'old_column' ] * 2 # Method 2 dataf_new = dataf . assign ( lambda d : d [ 'old_column' ] * 2 ) In many cases you might argue that method #2 is safer because you do not need to worry about the dataf.copy() that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use lambda functions because it cannot pickle the code.","title":"Don't use lambda"},{"location":"guide/function-preprocess/function-preprocessing/#dont-cheat","text":"The functions that you write are supposed to be stateless in the sense that they don't learn from the data that goes in. You could theoretically bypass this with global variables but by doing so you're doing yourself a disservice. If you do this you'll be cheating the statistics by leaking information.","title":"Don't Cheat!"},{"location":"en/","text":"Human Learn hahaha \u00b6 Machine Learning models should play by the rules, literally. Project Goal \u00b6 Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground but it is also capable of making bad decision. We've also reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This package contains scikit-learn compatible tools that should make it easier to construct and benchmark rule based systems that are designed by humans. You can also use it in combination with ML models. Install \u00b6 You can install this tool via pip . python - m pip install human - learn Guides \u00b6 Tutorial \u00b6 There is a full course on this tool available on calmcode.io . This is the first video. Getting Started \u00b6 To help you get started we've written some helpful getting started guides. Functions as a Model Human Preprocessing Drawing as a Model Outliers and Comfort Drawing Features You can also check out the API documentation here . Features \u00b6 This library hosts a couple of models that you can play with. Interactive Drawings \u00b6 This tool allows you to draw over your datasets. These drawings can later be converted to models or to preprocessing tools. Classification Models \u00b6 FunctionClassifier \u00b6 This allows you to define a function that can make classification predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. InteractiveClassifier \u00b6 This allows you to draw decision boundaries in interactive charts to create a model. You can create charts interactively in the notebook and export it as a scikit-learn compatible model. Regression Models \u00b6 FunctionRegressor \u00b6 This allows you to define a function that can make regression predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. Outlier Detection Models \u00b6 FunctionOutlierDetector \u00b6 This allows you to define a function that can declare outliers. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. InteractiveOutlierDetector \u00b6 This allows you to draw decision boundaries in interactive charts to create a model. If a point falls outside of these boundaries we might be able to declare it an outlier. There's a threshold parameter for how strict you might want to be. Preprocessing Models \u00b6 PipeTransformer \u00b6 This allows you to define a function that can make handle preprocessing. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. This is especially powerful in combination with the pandas .pipe method. If you're unfamiliar with this amazing feature, you may appreciate this tutorial . InteractivePreprocessor \u00b6 This allows you to draw features that you'd like to add to your dataset or your machine learning pipeline. You can use it via tfm.fit(df).transform(df) and df.pipe(tfm) . Datasets \u00b6 Titanic \u00b6 This library hosts the popular titanic survivor dataset for demo purposes. The goal of this dataset is to predict who might have survived the titanic disaster. Fish \u00b6 The fish market dataset is also hosted in this library. The goal of this dataset is to predict the weight of fish. However, it can also be turned into a classification problem by predicting the species.","title":"Index"},{"location":"en/#human-learn-hahaha","text":"Machine Learning models should play by the rules, literally.","title":"Human Learn hahaha"},{"location":"en/#project-goal","text":"Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground but it is also capable of making bad decision. We've also reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This package contains scikit-learn compatible tools that should make it easier to construct and benchmark rule based systems that are designed by humans. You can also use it in combination with ML models.","title":"Project Goal"},{"location":"en/#install","text":"You can install this tool via pip . python - m pip install human - learn","title":"Install"},{"location":"en/#guides","text":"","title":"Guides"},{"location":"en/#tutorial","text":"There is a full course on this tool available on calmcode.io . This is the first video.","title":"Tutorial"},{"location":"en/#getting-started","text":"To help you get started we've written some helpful getting started guides. Functions as a Model Human Preprocessing Drawing as a Model Outliers and Comfort Drawing Features You can also check out the API documentation here .","title":"Getting Started"},{"location":"en/#features","text":"This library hosts a couple of models that you can play with.","title":"Features"},{"location":"en/#interactive-drawings","text":"This tool allows you to draw over your datasets. These drawings can later be converted to models or to preprocessing tools.","title":"Interactive Drawings"},{"location":"en/#classification-models","text":"","title":"Classification Models"},{"location":"en/#functionclassifier","text":"This allows you to define a function that can make classification predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.","title":"FunctionClassifier"},{"location":"en/#interactiveclassifier","text":"This allows you to draw decision boundaries in interactive charts to create a model. You can create charts interactively in the notebook and export it as a scikit-learn compatible model.","title":"InteractiveClassifier"},{"location":"en/#regression-models","text":"","title":"Regression Models"},{"location":"en/#functionregressor","text":"This allows you to define a function that can make regression predictions. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.","title":"FunctionRegressor"},{"location":"en/#outlier-detection-models","text":"","title":"Outlier Detection Models"},{"location":"en/#functionoutlierdetector","text":"This allows you to define a function that can declare outliers. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search.","title":"FunctionOutlierDetector"},{"location":"en/#interactiveoutlierdetector","text":"This allows you to draw decision boundaries in interactive charts to create a model. If a point falls outside of these boundaries we might be able to declare it an outlier. There's a threshold parameter for how strict you might want to be.","title":"InteractiveOutlierDetector"},{"location":"en/#preprocessing-models","text":"","title":"Preprocessing Models"},{"location":"en/#pipetransformer","text":"This allows you to define a function that can make handle preprocessing. It's constructed in such a way that you can use the arguments of the function as a parameter that you can benchmark in a grid-search. This is especially powerful in combination with the pandas .pipe method. If you're unfamiliar with this amazing feature, you may appreciate this tutorial .","title":"PipeTransformer"},{"location":"en/#interactivepreprocessor","text":"This allows you to draw features that you'd like to add to your dataset or your machine learning pipeline. You can use it via tfm.fit(df).transform(df) and df.pipe(tfm) .","title":"InteractivePreprocessor"},{"location":"en/#datasets","text":"","title":"Datasets"},{"location":"en/#titanic","text":"This library hosts the popular titanic survivor dataset for demo purposes. The goal of this dataset is to predict who might have survived the titanic disaster.","title":"Titanic"},{"location":"en/#fish","text":"The fish market dataset is also hosted in this library. The goal of this dataset is to predict the weight of fish. However, it can also be turned into a classification problem by predicting the species.","title":"Fish"},{"location":"en/api/classification/","text":"from hulearn.classification import * \u00b6 ::: hulearn.classification.functionclassifier ::: hulearn.classification.interactiveclassifier","title":"Classification EN"},{"location":"en/api/classification/#from-hulearnclassification-import","text":"::: hulearn.classification.functionclassifier ::: hulearn.classification.interactiveclassifier","title":"from hulearn.classification import *"},{"location":"en/api/common/","text":"from hulearn.common import * \u00b6 ::: hulearn.common","title":"Common"},{"location":"en/api/common/#from-hulearncommon-import","text":"::: hulearn.common","title":"from hulearn.common import *"},{"location":"en/api/datasets/","text":"from hulearn.datasets import * \u00b6 ::: hulearn.datasets","title":"Datasets"},{"location":"en/api/datasets/#from-hulearndatasets-import","text":"::: hulearn.datasets","title":"from hulearn.datasets import *"},{"location":"en/api/interactive-charts/","text":"InteractiveCharts \u00b6 ::: hulearn.experimental.InteractiveCharts parallel_coordinates \u00b6 ::: hulearn.experimental.parallel_coordinates","title":"Charts"},{"location":"en/api/interactive-charts/#interactivecharts","text":"::: hulearn.experimental.InteractiveCharts","title":"InteractiveCharts"},{"location":"en/api/interactive-charts/#parallel_coordinates","text":"::: hulearn.experimental.parallel_coordinates","title":"parallel_coordinates"},{"location":"en/api/outlier/","text":"from hulearn.outlier import * \u00b6 ::: hulearn.outlier.functionoutlier ::: hulearn.outlier.interactiveoutlier","title":"Outlier"},{"location":"en/api/outlier/#from-hulearnoutlier-import","text":"::: hulearn.outlier.functionoutlier ::: hulearn.outlier.interactiveoutlier","title":"from hulearn.outlier import *"},{"location":"en/api/preprocessing/","text":"from hulearn.preprocessing import * \u00b6 ::: hulearn.preprocessing.pipetransformer ::: hulearn.preprocessing.interactivepreprocessor","title":"Preprocessing"},{"location":"en/api/preprocessing/#from-hulearnpreprocessing-import","text":"::: hulearn.preprocessing.pipetransformer ::: hulearn.preprocessing.interactivepreprocessor","title":"from hulearn.preprocessing import *"},{"location":"en/api/regression/","text":"from hulearn.regression import * \u00b6 ::: hulearn.regression.functionregressor","title":"Regression"},{"location":"en/api/regression/#from-hulearnregression-import","text":"::: hulearn.regression.functionregressor","title":"from hulearn.regression import *"},{"location":"en/api/rulers/","text":"CaseWhenRuler \u00b6 ::: hulearn.experimental.CaseWhenRuler","title":"Rulers"},{"location":"en/api/rulers/#casewhenruler","text":"::: hulearn.experimental.CaseWhenRuler","title":"CaseWhenRuler"},{"location":"en/examples/faq/","text":"Frequently Asked Questions \u00b6 Feel free to ask questions here . What are the Lessons Learned \u00b6 If you're interested in some of the lessons the creators of this tool learned while creating it, all you need to do is follow the python tradition. from hulearn import this Why Make This? \u00b6 Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground. But we've reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This made us wonder if we could make machine learning listen more to common sense. There's a lot of things that could go wrong otherwise. If you're interested in examples we might recommend this pydata talk . I'm getting a PORT error! \u00b6 You might get an error that looks like; ERROR : bokeh . server . views . ws : Refusing websocket connection from Origin 'http://localhost:8889' ; use -- allow - websocket - origin = localhost : 8889 or set BOKEH_ALLOW_WS_ORIGIN = localhost : 8889 to permit this ; currently we allow origins { 'localhost:8888' } This is related to something bokeh cannot do without explicit permission from jupyter. It can't be fixed by this library but you can circumvent this error by running jupyter via; python -m jupyter lab --port 8889 --allow-websocket-origin=localhost:8889 You can also set an environment variable BOKEH_ALLOW_WS_ORIGIN=localhost:8889 .","title":"FAQ EN"},{"location":"en/examples/faq/#frequently-asked-questions","text":"Feel free to ask questions here .","title":"Frequently Asked Questions"},{"location":"en/examples/faq/#what-are-the-lessons-learned","text":"If you're interested in some of the lessons the creators of this tool learned while creating it, all you need to do is follow the python tradition. from hulearn import this","title":"What are the Lessons Learned"},{"location":"en/examples/faq/#why-make-this","text":"Back in the old days, it was common to write rule-based systems. Systems that do; Nowadays, it's much more fashionable to use machine learning instead. Something like; We started wondering if we might have lost something in this transition. Sure, machine learning covers a lot of ground. But we've reached a stage of hype that folks forget that many classification problems can be handled by natural intelligence too. This made us wonder if we could make machine learning listen more to common sense. There's a lot of things that could go wrong otherwise. If you're interested in examples we might recommend this pydata talk .","title":"Why Make This?"},{"location":"en/examples/faq/#im-getting-a-port-error","text":"You might get an error that looks like; ERROR : bokeh . server . views . ws : Refusing websocket connection from Origin 'http://localhost:8889' ; use -- allow - websocket - origin = localhost : 8889 or set BOKEH_ALLOW_WS_ORIGIN = localhost : 8889 to permit this ; currently we allow origins { 'localhost:8888' } This is related to something bokeh cannot do without explicit permission from jupyter. It can't be fixed by this library but you can circumvent this error by running jupyter via; python -m jupyter lab --port 8889 --allow-websocket-origin=localhost:8889 You can also set an environment variable BOKEH_ALLOW_WS_ORIGIN=localhost:8889 .","title":"I'm getting a PORT error!"},{"location":"en/examples/model-mining/","text":"In this example, we will demonstrate that you can use visual data mining techniques to discover meaningful patterns in your data. These patterns can be easily translated into a machine learning model by using the tools found in this package. You can find a full tutorial of this technique on calmcode but the main video can be viewed below. The Task \u00b6 We're going to make a rule based model for the creditcard dataset. The main feature of the dataset is that it is suffering from a class imbalance. Instead of training a machine learning model, let's try to instead explore it with a parallel coordinates chart. If you scroll all the way to the bottom of this tutorial you'll see an example of such a chart. It shows a \"train\"-set. We explored the data just like in the video and that led us to define the following model. from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , age = 15 ): ruler = CaseWhenRuler ( default = 0 ) ( ruler . add_rule ( lambda d : ( d [ 'V11' ] > 4 ), 1 ) . add_rule ( lambda d : ( d [ 'V17' ] < - 3 ), 1 ) . add_rule ( lambda d : ( d [ 'V14' ] < - 8 ), 1 )) return ruler . predict ( dataf ) clf = FunctionClassifier ( make_prediction ) Full Code First we load the data. ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split df_credit = fetch_openml( data_id=1597, as_frame=True ) credit_train, credit_test = train_test_split(df_credit, test_size=0.5, shuffle=True) ``` Next, we create a hiplot in jupyter. ```python import json import hiplot as hip samples = [credit_train.loc[lambda d: d['group'] == True], credit_train.sample(5000)] json_data = pd.concat(samples).to_json(orient='records') hip.Experiment.from_iterable(json.loads(json_data)).display() ``` Given that we have our model, we can make a classification report. ```python from sklearn.metrics import classification_report Note that fit is a no-op here. \u00b6 preds = clf.fit(credit_train, credit_train['group']).predict(credit_test)) print(classification_report(credit_test['group'], preds) ``` When we ran the benchmark locally, we got the following classification report. precision recall f1-score support False 1.00 1.00 1.00 142165 True 0.70 0.73 0.71 239 accuracy 1.00 142404 macro avg 0.85 0.86 0.86 142404 weighted avg 1.00 1.00 1.00 142404 Deep Learning \u00b6 It's not a perfect benchmark, but we could compare this result to the one that's demonstrated on the keras blog . The trained model there lists 86.67% precision but only 23.9% recall. Depending on your preferences for false-positives, you could argue that our model is outperforming the deep learning model. It's not 100% a fair comparison. You can imagine that the keras blogpost is written to explain keras. The auther likely didn't attempt to make a state-of-the-art model. But what this demo does show is the merit of turning an exploratory data analysis into a model. You can end up with a very interpretable model, you might learn something about your data along the way and the model might certainly still perform well. Parallel Coordinates \u00b6 If you hover of the group name and right-click, you'll be able to set it for coloring and repeat the experience in the video. By doing that it becomes quite easy to eyeball how to separate the two classes. The V17 column especially seems powerful here. In real life we might ask \"why?\" this column is so distinctive but for now we'll just play around until we find a sensible model.","title":"Model Mining EN"},{"location":"en/examples/model-mining/#the-task","text":"We're going to make a rule based model for the creditcard dataset. The main feature of the dataset is that it is suffering from a class imbalance. Instead of training a machine learning model, let's try to instead explore it with a parallel coordinates chart. If you scroll all the way to the bottom of this tutorial you'll see an example of such a chart. It shows a \"train\"-set. We explored the data just like in the video and that led us to define the following model. from hulearn.classification import FunctionClassifier from hulearn.experimental import CaseWhenRuler def make_prediction ( dataf , age = 15 ): ruler = CaseWhenRuler ( default = 0 ) ( ruler . add_rule ( lambda d : ( d [ 'V11' ] > 4 ), 1 ) . add_rule ( lambda d : ( d [ 'V17' ] < - 3 ), 1 ) . add_rule ( lambda d : ( d [ 'V14' ] < - 8 ), 1 )) return ruler . predict ( dataf ) clf = FunctionClassifier ( make_prediction ) Full Code First we load the data. ```python from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split df_credit = fetch_openml( data_id=1597, as_frame=True ) credit_train, credit_test = train_test_split(df_credit, test_size=0.5, shuffle=True) ``` Next, we create a hiplot in jupyter. ```python import json import hiplot as hip samples = [credit_train.loc[lambda d: d['group'] == True], credit_train.sample(5000)] json_data = pd.concat(samples).to_json(orient='records') hip.Experiment.from_iterable(json.loads(json_data)).display() ``` Given that we have our model, we can make a classification report. ```python from sklearn.metrics import classification_report","title":"The Task"},{"location":"en/examples/model-mining/#note-that-fit-is-a-no-op-here","text":"preds = clf.fit(credit_train, credit_train['group']).predict(credit_test)) print(classification_report(credit_test['group'], preds) ``` When we ran the benchmark locally, we got the following classification report. precision recall f1-score support False 1.00 1.00 1.00 142165 True 0.70 0.73 0.71 239 accuracy 1.00 142404 macro avg 0.85 0.86 0.86 142404 weighted avg 1.00 1.00 1.00 142404","title":"Note that fit is a no-op here."},{"location":"en/examples/model-mining/#deep-learning","text":"It's not a perfect benchmark, but we could compare this result to the one that's demonstrated on the keras blog . The trained model there lists 86.67% precision but only 23.9% recall. Depending on your preferences for false-positives, you could argue that our model is outperforming the deep learning model. It's not 100% a fair comparison. You can imagine that the keras blogpost is written to explain keras. The auther likely didn't attempt to make a state-of-the-art model. But what this demo does show is the merit of turning an exploratory data analysis into a model. You can end up with a very interpretable model, you might learn something about your data along the way and the model might certainly still perform well.","title":"Deep Learning"},{"location":"en/examples/model-mining/#parallel-coordinates","text":"If you hover of the group name and right-click, you'll be able to set it for coloring and repeat the experience in the video. By doing that it becomes quite easy to eyeball how to separate the two classes. The V17 column especially seems powerful here. In real life we might ask \"why?\" this column is so distinctive but for now we'll just play around until we find a sensible model.","title":"Parallel Coordinates"},{"location":"en/examples/usage/","text":"This page contains a list of short examples that demonstrate the utility of the tools in this package. The goal for each example is to be small and consise. Precision and Subgroups \u00b6 It can be the case that for a subgroup of the population you do not need a model. Suppose that we have a session log dataset from \"World of Warcraft\". We know when people logged in, if they were part of a guild and when they stopped playing. You can create a machine learning model to predict which players are at risk of quitting the game but you might also be able to come up with some simple rules. Here is one rule that might work out swell: \"If any player was playing the video game at 24:00 on new-years eve, odds are that this person is very invested in the game and won't stop playing.\" This one rule will not cover the entire population but for the subgroup it can be an effective rule. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier classifier = SomeScikitLearnModel () def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # Override model prediction if a user is a heavy_user, no matter what res = np . where ( dataf [ 'heavy_user' ], \"stays\" , res ) return res fallback_model = FunctionClassifier ( make_decision ) No Data No Problem \u00b6 Let's say that we're interested in detecting fraud at a tax office. Even without looking at the data we can already come up with some sensible rules. Any minor making over the median income is \"suspicious\". Any person who started more than 2 companies in a year is \"suspicious\". Any person who has more than 10 bank accounts is \"suspicious\". The thing with these rules is that they are easy to explain but they are not based on data at all. In fact, they may not occur in the data at all. This means that a machine learning model may not have picked up this pattern that we're interested in. Thankfully, the lack in data can be compensated with business rules. Comfort Zone \u00b6 Models typically have a \"comfort zone\". If a new data point comes in that is very different from what the models saw before it should not be treated the same way. You can also argue that points with low proba score should also not be automated. If you want to prevent predictions where the model is \"unsure\" then you might want to follow this diagram; You can construct such a system by creating a FunctionClassifier that handles the logic you require. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier # We're importing a classifier/outlier detector from our library # but nothing is stopping you from using those in scikit-learn. # Just make sure that they are trained beforehand! outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision ) For more information on why this tactic is helpful: blogpost pydata talk","title":"Usage"},{"location":"en/examples/usage/#precision-and-subgroups","text":"It can be the case that for a subgroup of the population you do not need a model. Suppose that we have a session log dataset from \"World of Warcraft\". We know when people logged in, if they were part of a guild and when they stopped playing. You can create a machine learning model to predict which players are at risk of quitting the game but you might also be able to come up with some simple rules. Here is one rule that might work out swell: \"If any player was playing the video game at 24:00 on new-years eve, odds are that this person is very invested in the game and won't stop playing.\" This one rule will not cover the entire population but for the subgroup it can be an effective rule. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier classifier = SomeScikitLearnModel () def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # Override model prediction if a user is a heavy_user, no matter what res = np . where ( dataf [ 'heavy_user' ], \"stays\" , res ) return res fallback_model = FunctionClassifier ( make_decision )","title":"Precision and Subgroups"},{"location":"en/examples/usage/#no-data-no-problem","text":"Let's say that we're interested in detecting fraud at a tax office. Even without looking at the data we can already come up with some sensible rules. Any minor making over the median income is \"suspicious\". Any person who started more than 2 companies in a year is \"suspicious\". Any person who has more than 10 bank accounts is \"suspicious\". The thing with these rules is that they are easy to explain but they are not based on data at all. In fact, they may not occur in the data at all. This means that a machine learning model may not have picked up this pattern that we're interested in. Thankfully, the lack in data can be compensated with business rules.","title":"No Data No Problem"},{"location":"en/examples/usage/#comfort-zone","text":"Models typically have a \"comfort zone\". If a new data point comes in that is very different from what the models saw before it should not be treated the same way. You can also argue that points with low proba score should also not be automated. If you want to prevent predictions where the model is \"unsure\" then you might want to follow this diagram; You can construct such a system by creating a FunctionClassifier that handles the logic you require. As an illustrative example we'll implement this diagram as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier # We're importing a classifier/outlier detector from our library # but nothing is stopping you from using those in scikit-learn. # Just make sure that they are trained beforehand! outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision ) For more information on why this tactic is helpful: blogpost pydata talk","title":"Comfort Zone"},{"location":"en/guide/drawing-classifier/drawing%20copy/","text":"Drawing as a Model \u00b6 Classic Classification Problem \u00b6 Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier. Let's Draw! \u00b6 Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas. Instructions \u00b6 To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model? Properties of Modelling Technique \u00b6 Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model. From Jupyter \u00b6 In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this; Serialize \u00b6 You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made. Model \u00b6 This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API. Conclusion \u00b6 The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems. Notebook \u00b6 If you want to run this code yourself, feel free to download the notebook .","title":"Drawing as a Model"},{"location":"en/guide/drawing-classifier/drawing%20copy/#drawing-as-a-model","text":"","title":"Drawing as a Model"},{"location":"en/guide/drawing-classifier/drawing%20copy/#classic-classification-problem","text":"Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier.","title":"Classic Classification Problem"},{"location":"en/guide/drawing-classifier/drawing%20copy/#lets-draw","text":"Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas.","title":"Let's Draw!"},{"location":"en/guide/drawing-classifier/drawing%20copy/#instructions","text":"To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model?","title":"Instructions"},{"location":"en/guide/drawing-classifier/drawing%20copy/#properties-of-modelling-technique","text":"Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model.","title":"Properties of Modelling Technique"},{"location":"en/guide/drawing-classifier/drawing%20copy/#from-jupyter","text":"In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this;","title":"From Jupyter"},{"location":"en/guide/drawing-classifier/drawing%20copy/#serialize","text":"You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made.","title":"Serialize"},{"location":"en/guide/drawing-classifier/drawing%20copy/#model","text":"This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API.","title":"Model"},{"location":"en/guide/drawing-classifier/drawing%20copy/#conclusion","text":"The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems.","title":"Conclusion"},{"location":"en/guide/drawing-classifier/drawing%20copy/#notebook","text":"If you want to run this code yourself, feel free to download the notebook .","title":"Notebook"},{"location":"en/guide/drawing-classifier/drawing/","text":"Drawing as a Model \u00b6 Classic Classification Problem \u00b6 Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier. Let's Draw! \u00b6 Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas. Instructions \u00b6 To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model? Properties of Modelling Technique \u00b6 Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model. From Jupyter \u00b6 In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this; Serialize \u00b6 You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made. Model \u00b6 This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API. Conclusion \u00b6 The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems. Notebook \u00b6 If you want to run this code yourself, feel free to download the notebook .","title":"Drawing as a Model EN"},{"location":"en/guide/drawing-classifier/drawing/#drawing-as-a-model","text":"","title":"Drawing as a Model"},{"location":"en/guide/drawing-classifier/drawing/#classic-classification-problem","text":"Let's look at a dataset that describes a classification problem. In particular, we're looking at the pallmer penguin dataset here. The goal is to predict the colors of the points. Very commonly folks would look at this and say; But maybe, this is a wrong interpretation. Maybe the problem isn't the fact that as a human we can't split up the points. Instead the problem here is that code is not the best user-interface. Sure, writing the code to split the points is hard but if we could just draw, it'd be much easier.","title":"Classic Classification Problem"},{"location":"en/guide/drawing-classifier/drawing/#lets-draw","text":"Because we've got the web at our disposal and tools like bokeh we can also turn the static chart into an interactive one. The nice thing about interactive charts is that we can interact with them. The chart below allows you to draw on the canvas.","title":"Let's Draw!"},{"location":"en/guide/drawing-classifier/drawing/#instructions","text":"To draw, you first need to pick a color. Then you can double click in the canvas to start drawing a polygon. Once you're done drawing you can double click again to stop shaping the polygon. A drawn polygon can be moved by clicking and dragging. You can delete a polygon by clicking it once and hitting backspace. You can also edit it by clicking the edit button (immediately under the green button). You can delete a polygon by first clicking the polygon once and then hitting backspace. Once you're done drawing you might end up with a drawing that looks like this. When you look at it such a drawing. It makes you wonder, wouldn't it be nice if this was the output of a machine learning model?","title":"Instructions"},{"location":"en/guide/drawing-classifier/drawing/#properties-of-modelling-technique","text":"Instead of doing machine learning we're doing \"human learning\" here. We can literally draw out what we think the computer should predict and there's some interesting benefits to consider. By drawing on the data, you're immediately forced to understand it. The act of modelling now also includes the act of exploratory analysis. By drawing the model, you immediately interpret and understand it better. This is great when you think about themes like fairness. It's still not 100% perfect but the added interpretability should make it a lot easier to prevent artificial stupidity. You can draw on the canvas, even if there's no data! This is something that machine learning algorithms typically have the worst time ever with. If you're doing fraud modelling, then you can manually specify a region to be \"risky\" even when there is no data for it! You can draw on the canvas, even if there's no labels! You might be able to come up with a \"common sense\" drawing even if there are no labels available. The model will be fully heuristic based, but perhaps still useful. We can interpret the drawing in many ways. Maybe if you've not drawn a region we can interpret it as \"wont predict here\". This can be a like-able safety mechanism. If nothing else, these drawings should serve as a lovely benchmark. If the performance of your deep ensemble model isn't significantly better than a drawn model, then you may not need the complex model.","title":"Properties of Modelling Technique"},{"location":"en/guide/drawing-classifier/drawing/#from-jupyter","text":"In reality one 2D chart is probably not going to cut it. So in a jupyter notebook you can drawn many! Here's how it works. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The clf variable contains a InteractiveCharts object that has assumed that the \"species\" column in df to represent the label that we're interested in. From here you can generate charts, via; # It's best to run this in a single cell. clf . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) You can also generate a second chart. # Again, run this in a seperate cell. clf . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) This will generate two interactive charts where you can \"draw\" you model. The final drawn result might look something like this;","title":"From Jupyter"},{"location":"en/guide/drawing-classifier/drawing/#serialize","text":"You can translate these decision boundaries to a machine learning model if you want. To do that you first need to translate your drawings to json. drawn_data = clf . data () # You can also save the data to disk if you want. clf . to_json ( \"drawn-model.json\" ) What the json file looks like. [{ 'chart_id' : '3c680a70-0' , 'x' : 'bill_length_mm' , 'y' : 'bill_depth_mm' , 'polygons' : { 'Adelie' : { 'bill_length_mm' : [[ 32.14132787891895 , 32.84074984423687 , 38.78583654943918 , 46.829189150595255 , 47.17890013325422 , 43.68179030666462 , 35.63843770550855 ]], 'bill_depth_mm' : [[ 15.406862190509665 , 19.177207018095874 , 21.487207018095873 , 21.5934139146476 , 19.217943123601575 , 16.640631196069247 , 15.244587235322568 ]]}, 'Gentoo' : { 'bill_length_mm' : [[ 58.10736834134671 , 50.501154468514336 , 40.18468048007502 , 40.09725273441028 , 44.556067763312015 , 53.12398683845653 , 58.894218052329364 , 60.76142402357685 ]], 'bill_depth_mm' : [[ 17.284959177952327 , 17.553429170403614 , 14.627106252684614 , 13.201081726611287 , 12.051605398390103 , 13.827533449580619 , 15.667347786949287 , 17.024587871893388 ]]}, 'Chinstrap' : { 'bill_length_mm' : [[ 44.11892903498832 , 40.88410244539294 , 45.51777296562416 , 51.72514290782069 , 56.621096665046124 , 58.019940595681966 , 53.29884232978601 , 52.511992618803355 , 47.004044641924736 ]], 'bill_depth_mm' : [[ 16.103691211166677 , 16.72117219380463 , 19.217943123601575 , 20.85561007755441 , 21.124080070005693 , 19.540107114543115 , 18.57361514171849 , 16.39900820286309 , 15.915762216450778 ]]}}}, { 'chart_id' : '198b23fb-5' , 'x' : 'flipper_length_mm' , 'y' : 'body_mass_g' , 'polygons' : { 'Adelie' : { 'flipper_length_mm' : [[ 205.39985750238202 , 205.39985750238202 , 184.0772104628077 , 174.80649435864495 , 170.235872105095 , 161.6171609214579 , 174.42229536301556 , 194.38496200094178 , 197.57898866300997 , 209.5565886457657 , 204.4993797641577 ]], 'body_mass_g' : [[ 4079.2264346061725 , 4876.092877056334 , 4876.092877056334 , 4067.842628285456 , 3521.4199248910595 , 3088.8352847038286 , 2781.4725140444807 , 2781.4725140444807 , 3134.370509986695 , 3555.5713438532093 , 3737.7122449846747 ]]}, 'Gentoo' : { 'flipper_length_mm' : [[ 208.77192413146238 , 201.53909280616116 , 216.39571931218526 , 232.7342009323645 , 241.9517683831975 , 222.55068229508308 ]], 'body_mass_g' : [[ 3898.03455221242 , 4740.103517729661 , 6171.487627453468 , 6230.793172902075 , 5650.448345315868 , 4603.5517935917305 ]]}, 'Chinstrap' : { 'flipper_length_mm' : [[ 215.1341094117529 , 195.202069787803 , 173.41588694302055 , 181.06422772895482 , 197.75151671644775 , 212.35289458050406 ]], 'body_mass_g' : [[ 4330.448345315868 , 4626.310414281385 , 3272.1724832469026 , 2698.5776834613475 , 2872.5429646102134 , 3646.641794418942 ]]}}}] This data represents the drawings that you've made.","title":"Serialize"},{"location":"en/guide/drawing-classifier/drawing/#model","text":"This generated data can be read in by our InteractiveClassifier which will allow you to use your drawn model as a scikit-learn model. from hulearn.classification import InteractiveClassifier model = InteractiveClassifier ( json_desc = drawn_data ) # Alternatively you can also load from disk. InteractiveClassifier . from_json ( \"drawn-model.json\" ) This model can be used to make predictions but you will still need to follow the standard .fit(X, y) and .predict(X) pattern. X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict_proba ( X ) We can confirm that it has picked up the pattern that we drew too! The charts below show the predicted values preds plotted over the original charts that we drew. Code for the plots. import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'bill_length_mm' ], X [ 'bill_depth_mm' ], c = preds [:, i ]) plt . xlabel ( 'bill_length_mm' ) plt . ylabel ( 'bill_depth_mm' ) plt . title ( model . classes_ [ i ]) import matplotlib.pylab as plt plt . figure ( figsize = ( 12 , 3 )) for i in range ( 3 ): plt . subplot ( 131 + i ) plt . scatter ( X [ 'flipper_length_mm' ], X [ 'body_mass_g' ], c = preds [:, i ]) plt . xlabel ( 'flipper_length_mm' ) plt . ylabel ( 'body_mass_g' ) plt . title ( model . classes_ [ i ]) Because we've been drawing on two charts you should notice that the predictions won't match our drawings 100%. Internally we check if a point falls into a drawn polygon and a single point typically fits into more than a single polygon. If a point does not fit into any polygon then we assign a flat probability value to it. The details of how points in polygons are weighted will be explored with hyperparemters that will be added to the API.","title":"Model"},{"location":"en/guide/drawing-classifier/drawing/#conclusion","text":"The goal of this library is to make it easier to apply common sense to construct models. By thinking more in terms of \"human learning\" as opposed to \"machine learning\" you might be able to make models that are guaranteed to follow the rules. Is this way of modelling perfect? No. Human made rules can also be biased and we should also consider that this model still needs to undergo testing via a validation set. You still need to \"think\" when designing rule based systems.","title":"Conclusion"},{"location":"en/guide/drawing-classifier/drawing/#notebook","text":"If you want to run this code yourself, feel free to download the notebook .","title":"Notebook"},{"location":"en/guide/drawing-features/custom-features/","text":"Sofar we've explored drawing as a tool for models, but it can also be used as a tool to generate features. To explore this, let's load in the penguins dataset again. from sklego.datasets import load_penguins test df = load_penguins ( as_frame = True ) . dropna () Drawing \u00b6 We can draw over this dataset. It's like before but with one crucial differenc from hulearn.experimental.interactive import InteractiveCharts # Note that the `labels` arugment here is a list, not a string! This # tells the tool that we want to be able to add custom groups that are # not defined by a column in the dataframe. charts = InteractiveCharts ( df , labels = [ 'group_one' , 'group_two' ]) Let's make a custom drawing. charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) Let's assume the new drawing looks something like this. Sofar these drawn features have been used to construct models. But they can also be used to help label data or generate extra features for machine learning models. Features \u00b6 This library makes it easy to add these features to scikit-learn pipelines or to pandas. To get started, you'll want to import the InteractivePreprocessor . from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor ( json_desc = charts . data ()) This tfm object is can be used as a preprocessing step inside of scikit-learn but it can also be used in a pandas pipeline. # The flow for scikit-learn tfm . fit ( df ) . transform ( df ) # The flow for pandas df . pipe ( tfm . pandas_pipe )","title":"Drawing Features EN"},{"location":"en/guide/drawing-features/custom-features/#drawing","text":"We can draw over this dataset. It's like before but with one crucial differenc from hulearn.experimental.interactive import InteractiveCharts # Note that the `labels` arugment here is a list, not a string! This # tells the tool that we want to be able to add custom groups that are # not defined by a column in the dataframe. charts = InteractiveCharts ( df , labels = [ 'group_one' , 'group_two' ]) Let's make a custom drawing. charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) Let's assume the new drawing looks something like this. Sofar these drawn features have been used to construct models. But they can also be used to help label data or generate extra features for machine learning models.","title":"Drawing"},{"location":"en/guide/drawing-features/custom-features/#features","text":"This library makes it easy to add these features to scikit-learn pipelines or to pandas. To get started, you'll want to import the InteractivePreprocessor . from hulearn.preprocessing import InteractivePreprocessor tfm = InteractivePreprocessor ( json_desc = charts . data ()) This tfm object is can be used as a preprocessing step inside of scikit-learn but it can also be used in a pandas pipeline. # The flow for scikit-learn tfm . fit ( df ) . transform ( df ) # The flow for pandas df . pipe ( tfm . pandas_pipe )","title":"Features"},{"location":"en/guide/finding-outliers/outliers/","text":"Rethinking Classification \u00b6 Let's have another look at the interactive canvas that we saw in the previous guide. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The drawn colors indicate that a human deemed a classification appropriate. You could wonder what we might want to do with the regions that have not been colored though. Machine learning algorithms might typically still assign a class to those regions but that can be a dangerous idea. We might consider these points outside of the \"comfort zone\" of the predicted areas. In these situations it might be best to declare it an outlier and to handle it differently. That way we don't automate a decision that we're likely to regret later. Outliers \u00b6 The drawn charts can be used to construct a classifier but they may also be used to construct an outlier detection model. This allows us to re-use earlier work for multiple purposes. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () charts = InteractiveCharts ( df , labels = \"species\" ) # Run this in a seperate cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Run this in a seperate cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) To demonstrate how it works, let's assume that we've drawn the following: We'll again fetch the drawn data but now we'll use it to detect outliers. from hulearn.outlier import InteractiveOutlierDetector # Load the model using drawn-data. model = InteractiveOutlierDetector ( json_desc = charts . data ()) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict ( X ) This model can now be used as a scikit-learn compatible outlier detection model. Here's the output of the model. Code for the plots. ```python import matplotlib.pylab as plt plt.figure(figsize=(10, 4)) plt.subplot(121) plt.scatter(X['bill_length_mm'], X['bill_depth_mm'], c=preds) plt.xlabel('bill_length_mm') plt.ylabel('bill_depth_mm') plt.subplot(122) plt.scatter(X['flipper_length_mm'], X['body_mass_g'], c=preds) plt.xlabel('flipper_length_mm') plt.ylabel('body_mass_g'); ``` How it works. \u00b6 A point is considered an outlier if it does not fall inside of enough drawn polygons. The number of poylgons that a point must fall into is a parameter that you can set manually or even search for in a grid-search. For example, let's repeat the exercise. The base setting is that a point needs to be in at least one polygon but we can change this to two. # Before model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 1 ) # After model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 2 ) Combine \u00b6 You might wonder, can we combine the FunctionClassifier with an outlier model like we've got here? Yes! Use a FunctionClassifier ! As an illustrative example we'll implement a diagram like above as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision )","title":"Outliers and Comfort EN"},{"location":"en/guide/finding-outliers/outliers/#rethinking-classification","text":"Let's have another look at the interactive canvas that we saw in the previous guide. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () clf = InteractiveCharts ( df , labels = \"species\" ) The drawn colors indicate that a human deemed a classification appropriate. You could wonder what we might want to do with the regions that have not been colored though. Machine learning algorithms might typically still assign a class to those regions but that can be a dangerous idea. We might consider these points outside of the \"comfort zone\" of the predicted areas. In these situations it might be best to declare it an outlier and to handle it differently. That way we don't automate a decision that we're likely to regret later.","title":"Rethinking Classification"},{"location":"en/guide/finding-outliers/outliers/#outliers","text":"The drawn charts can be used to construct a classifier but they may also be used to construct an outlier detection model. This allows us to re-use earlier work for multiple purposes. from sklego.datasets import load_penguins from hulearn.experimental.interactive import InteractiveCharts df = load_penguins ( as_frame = True ) . dropna () charts = InteractiveCharts ( df , labels = \"species\" ) # Run this in a seperate cell charts . add_chart ( x = \"bill_length_mm\" , y = \"bill_depth_mm\" ) # Run this in a seperate cell charts . add_chart ( x = \"flipper_length_mm\" , y = \"body_mass_g\" ) To demonstrate how it works, let's assume that we've drawn the following: We'll again fetch the drawn data but now we'll use it to detect outliers. from hulearn.outlier import InteractiveOutlierDetector # Load the model using drawn-data. model = InteractiveOutlierDetector ( json_desc = charts . data ()) X , y = df . drop ( columns = [ 'species' ]), df [ 'species' ] preds = model . fit ( X , y ) . predict ( X ) This model can now be used as a scikit-learn compatible outlier detection model. Here's the output of the model. Code for the plots. ```python import matplotlib.pylab as plt plt.figure(figsize=(10, 4)) plt.subplot(121) plt.scatter(X['bill_length_mm'], X['bill_depth_mm'], c=preds) plt.xlabel('bill_length_mm') plt.ylabel('bill_depth_mm') plt.subplot(122) plt.scatter(X['flipper_length_mm'], X['body_mass_g'], c=preds) plt.xlabel('flipper_length_mm') plt.ylabel('body_mass_g'); ```","title":"Outliers"},{"location":"en/guide/finding-outliers/outliers/#how-it-works","text":"A point is considered an outlier if it does not fall inside of enough drawn polygons. The number of poylgons that a point must fall into is a parameter that you can set manually or even search for in a grid-search. For example, let's repeat the exercise. The base setting is that a point needs to be in at least one polygon but we can change this to two. # Before model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 1 ) # After model = InteractiveOutlierDetector ( json_desc = charts . data (), threshold = 2 )","title":"How it works."},{"location":"en/guide/finding-outliers/outliers/#combine","text":"You might wonder, can we combine the FunctionClassifier with an outlier model like we've got here? Yes! Use a FunctionClassifier ! As an illustrative example we'll implement a diagram like above as a Classifier . import numpy as np from hulearn.outlier import InteractiveOutlierDetector from hulearn.classification import FunctionClassifier , InteractiveClassifier outlier = InteractiveOutlierDetector . from_json ( \"path/to/file.json\" ) classifier = InteractiveClassifier . from_json ( \"path/to/file.json\" ) def make_decision ( dataf ): # First we create a resulting array with all the predictions res = classifier . predict ( dataf ) # If we detect doubt, \"classify\" it as a fallback instead. proba = classifier . predict_proba ( dataf ) res = np . where ( proba . max ( axis = 1 ) < 0.8 , \"doubt_fallback\" , res ) # If we detect an ourier, we'll fallback too. res = np . where ( outlier . predict ( dataf ) == - 1 , \"outlier_fallback\" , res ) # This `res` array contains the output of the drawn diagram. return res fallback_model = FunctionClassifier ( make_decision )","title":"Combine"},{"location":"en/guide/function-classifier/function-classifier/","text":"The goal of this library is to make it easier to declare common sense models. A very pythonic way of getting there is to declare a function. One of the first features in this library is the ability to re-use functions as if they are scikit-learn models. Titanic \u00b6 Let's see how this might work. We'll grab a dataset that is packaged along with this library. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) df . head () The df variable represents a dataframe and it has the following contents: survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe. Preparation \u00b6 To prepare our data we will first get it into the common X , y format for scikit-learn. X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense? It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better. FunctionClassifier \u00b6 Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the FunctionClassifier . You can see an example of that below. import numpy as np from hulearn.classification import FunctionClassifier def fare_based ( dataf , threshold = 10 ): \"\"\" The assumption is that folks who paid more are wealthier and are more likely to have recieved access to lifeboats. \"\"\" return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) mod = FunctionClassifier ( fare_based ) This mod is a scikit-learn model, which means that you can .fit(X, y).predict(X) . mod . fit ( X , y ) . predict ( X ) During the .fit(X, y) -step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step. Grid \u00b6 Being able to .fit(X, y).predict(X) is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best threshold value? For that, you might like to use GridSearchCV . from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score , recall_score , accuracy_score , make_scorer # Note the threshold keyword argument in this function. def fare_based ( dataf , threshold = 10 ): return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) # Pay attention here, we set the threshold argument in here. mod = FunctionClassifier ( fare_based , threshold = 10 ) # The GridSearch object can now \"grid-search\" over this argument. # We also add a bunch of metrics to our approach so we can measure. grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'threshold' : np . linspace ( 0 , 100 , 30 )}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) If we make a chart of the grid.cv_results_ then they would look something like; A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a RandomForestClassifier using the 'pclass', 'sex', 'age', 'fare' columns that the precision score would be about the same. Bigger Grids \u00b6 You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that. def last_name ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) # Once again, remember to declare your arguments here too! mod = FunctionClassifier ( last_name , pclass = 10 , sex = 'male' ) # The arguments of the function can now be \"grid-searched\". grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) Guidance \u00b6 Human Learn doesn't just allow you to turn functions into classifiers. It also tries to help you find rules that could be useful. In particular, an interactive parallel coordinates chart could be very helpful here. You can create a parallel coordinates chart directly inside of jupyter. from hulearn.experimental.interactive import parallel_coordinates parallel_coordinates ( df , label = \"survived\" , height = 200 ) What follows next are some explorations of the dataset. They are based on the scene from the titanic movie where they yell \"Woman and Children First!\". So let's see if we can confirm if this holds true. Explore \u00b6 It indeed seems that women in 1st/2nd class have a high chance of surviving. It also seems that male children have an increased change of survival, but only if they were travelling 1st/2nd class. Grid \u00b6 Here's a lovely observation. By doing exploratory analysis we not only understand the data better but we can now also turn the patterns that we've observed into a model! def make_prediction ( dataf , age = 15 ): women_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'sex' ] == \"female\" ) children_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'age' ] <= age ) return women_rule | children_rule mod = FunctionClassifier ( make_prediction ) We're even able to use grid-search again to find the optimal threshold for \"age\" . Comparison \u00b6 To compare our results we've also trained a RandomForestClassifier . Here's how the models compare; Model accuracy precision recall Women & Children Rule 0.808157 0.952168 0.558621 RandomForestClassifier 0.813869 0.785059 0.751724 It seems like our rule based model is quite reasonable. A great follow-up exercise would be to try and understand when the random forest model disagrees with the rule based system. This could lead us to understand more patterns in the data. Conclusion \u00b6 In this guide we've seen the FunctionClassifier in action. It is one of the many models in this library that will help you construct more \"human\" models. This component is very effective when it is combined with exploratory data analysis techniques. Notebook \u00b6 If you want to download with this code yourself, feel free to download the notebook here .","title":"Function as a Model EN"},{"location":"en/guide/function-classifier/function-classifier/#titanic","text":"Let's see how this might work. We'll grab a dataset that is packaged along with this library. from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) df . head () The df variable represents a dataframe and it has the following contents: survived pclass sex age fare sibsp 0 3 male 22 7.25 1 1 1 female 38 71.2833 1 1 3 female 26 7.925 0 1 1 female 35 53.1 1 0 3 male 35 8.05 0 There's actually some more columns in this dataset but we'll limit ourselves to just these for now. The goal of the dataset is to predict if you survived the titanic disaster based on the other attributes in this dataframe.","title":"Titanic"},{"location":"en/guide/function-classifier/function-classifier/#preparation","text":"To prepare our data we will first get it into the common X , y format for scikit-learn. X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] We could now start to import fancy machine learning models. It's what a lot of people do. Import a random forest, and see how high we can get the accuracy statistics. The goal of this library is to do the exact opposite. It might be a better idea to create a simple benchmark using, well, common sense? It's the goal of this library to make this easier for scikit-learn. In part because this helps us get to sensible benchmarks but also because this exercise usually makes you understand the data a whole lot better.","title":"Preparation"},{"location":"en/guide/function-classifier/function-classifier/#functionclassifier","text":"Let's write a simple python function that determines if you survived based on the amount of money you paid for your ticket. It might serve as a proxy for your survival rate. To get such a model to act as a scikit-learn model you can use the FunctionClassifier . You can see an example of that below. import numpy as np from hulearn.classification import FunctionClassifier def fare_based ( dataf , threshold = 10 ): \"\"\" The assumption is that folks who paid more are wealthier and are more likely to have recieved access to lifeboats. \"\"\" return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) mod = FunctionClassifier ( fare_based ) This mod is a scikit-learn model, which means that you can .fit(X, y).predict(X) . mod . fit ( X , y ) . predict ( X ) During the .fit(X, y) -step there's actually nothing being \"trained\" but it's a scikit-learn formality that every model has a \"fit\"-step and a \"predict\"-step.","title":"FunctionClassifier"},{"location":"en/guide/function-classifier/function-classifier/#grid","text":"Being able to .fit(X, y).predict(X) is nice. We could compare the predictions with the true values to get an idea of how well our heuristic works. But how do we know if we've picked the best threshold value? For that, you might like to use GridSearchCV . from sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score , recall_score , accuracy_score , make_scorer # Note the threshold keyword argument in this function. def fare_based ( dataf , threshold = 10 ): return np . array ( dataf [ 'fare' ] > threshold ) . astype ( int ) # Pay attention here, we set the threshold argument in here. mod = FunctionClassifier ( fare_based , threshold = 10 ) # The GridSearch object can now \"grid-search\" over this argument. # We also add a bunch of metrics to our approach so we can measure. grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'threshold' : np . linspace ( 0 , 100 , 30 )}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y ) If we make a chart of the grid.cv_results_ then they would look something like; A precision of 80% is not bad! It confirms our hunch that the folks who paid more for their ticket (potentially those in 1st class) had a better chance of surviving. An interesting thing to mention is that if you were to train a RandomForestClassifier using the 'pclass', 'sex', 'age', 'fare' columns that the precision score would be about the same.","title":"Grid"},{"location":"en/guide/function-classifier/function-classifier/#bigger-grids","text":"You can also come up with bigger grids that use multiple arguments of the function. We totally allow for that. def last_name ( dataf , sex = 'male' , pclass = 1 ): predicate = ( dataf [ 'sex' ] == sex ) & ( dataf [ 'pclass' ] == pclass ) return np . array ( predicate ) . astype ( int ) # Once again, remember to declare your arguments here too! mod = FunctionClassifier ( last_name , pclass = 10 , sex = 'male' ) # The arguments of the function can now be \"grid-searched\". grid = GridSearchCV ( mod , cv = 2 , param_grid = { 'pclass' : [ 1 , 2 , 3 ], 'sex' : [ 'male' , 'female' ]}, scoring = { 'accuracy' : make_scorer ( accuracy_score ), 'precision' : make_scorer ( precision_score ), 'recall' : make_scorer ( recall_score )}, refit = 'accuracy' ) grid . fit ( X , y )","title":"Bigger Grids"},{"location":"en/guide/function-classifier/function-classifier/#guidance","text":"Human Learn doesn't just allow you to turn functions into classifiers. It also tries to help you find rules that could be useful. In particular, an interactive parallel coordinates chart could be very helpful here. You can create a parallel coordinates chart directly inside of jupyter. from hulearn.experimental.interactive import parallel_coordinates parallel_coordinates ( df , label = \"survived\" , height = 200 ) What follows next are some explorations of the dataset. They are based on the scene from the titanic movie where they yell \"Woman and Children First!\". So let's see if we can confirm if this holds true.","title":"Guidance"},{"location":"en/guide/function-classifier/function-classifier/#explore","text":"It indeed seems that women in 1st/2nd class have a high chance of surviving. It also seems that male children have an increased change of survival, but only if they were travelling 1st/2nd class.","title":"Explore"},{"location":"en/guide/function-classifier/function-classifier/#grid_1","text":"Here's a lovely observation. By doing exploratory analysis we not only understand the data better but we can now also turn the patterns that we've observed into a model! def make_prediction ( dataf , age = 15 ): women_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'sex' ] == \"female\" ) children_rule = ( dataf [ 'pclass' ] < 3.0 ) & ( dataf [ 'age' ] <= age ) return women_rule | children_rule mod = FunctionClassifier ( make_prediction ) We're even able to use grid-search again to find the optimal threshold for \"age\" .","title":"Grid"},{"location":"en/guide/function-classifier/function-classifier/#comparison","text":"To compare our results we've also trained a RandomForestClassifier . Here's how the models compare; Model accuracy precision recall Women & Children Rule 0.808157 0.952168 0.558621 RandomForestClassifier 0.813869 0.785059 0.751724 It seems like our rule based model is quite reasonable. A great follow-up exercise would be to try and understand when the random forest model disagrees with the rule based system. This could lead us to understand more patterns in the data.","title":"Comparison"},{"location":"en/guide/function-classifier/function-classifier/#conclusion","text":"In this guide we've seen the FunctionClassifier in action. It is one of the many models in this library that will help you construct more \"human\" models. This component is very effective when it is combined with exploratory data analysis techniques.","title":"Conclusion"},{"location":"en/guide/function-classifier/function-classifier/#notebook","text":"If you want to download with this code yourself, feel free to download the notebook here .","title":"Notebook"},{"location":"en/guide/function-preprocess/function-preprocessing/","text":"In python the most popular data analysis tool is pandas while the most popular tool for making models is scikit-learn. We love the data wrangling tools of pandas while we appreciate the benchmarking capability of scikit-learn. The fact that these tools don't fully interact is slightly awkward. The data going into the model has an big effect on the output. So how might we more easily combine the two? Pipe \u00b6 In pandas there's an amazing trick that you can do with the .pipe method. We'll give a quick overview on how it works but if you're new to this idea you may appreciate this resource or this blogpost . from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] X . head ( 4 ) The goal of the titanic dataset is to predict weather or not a passenger survived the disaster. The X variable represents a dataframe with variables that we're going to use to predict survival (stored in y ). Here's a preview of what X might have. pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code. X [ 'nchar' ] = X [ 'name' ] . str . len () This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function. def process ( dataf ): # Make a copy of the dataframe to prevent it from overwriting the original data. dataf = dataf . copy () # Make the changes dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () # Return the name dataframe return dataf We now have a nice function that makes our changes and we can use it like so; X_new = process ( X ) We can do something more powerful though. Paramaters \u00b6 Let's make some more changes to our process function. def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end. We've changed the way we've defined our function but we're also changing the way that we're going to apply it. # This is equivalent to preprocessing(X) X . pipe ( preprocessing ) The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview. With .pipe() \u00b6 ( df . pipe ( set_col_types ) . pipe ( preprocessing , nchar = True , gender = False ) . pipe ( add_time_info )) Without .pipe() \u00b6 add_time_info ( preprocessing ( set_col_types ( df ), nchar = True , gender = False )) Let's be honest, this looks messy. PipeTransformer \u00b6 It would be great if we could use the preprocessing -function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas .pipe -line in general! For that we've got another feature in our library, the PipeTransformer . from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) # Important, don't forget to declare `n_char` and `gender` here. tfm = PipeTransformer ( preprocessing , n_char = True , gender = True )) The tfm variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function. from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV pipe = Pipeline ([ ( 'prep' , tfm ), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) Once trained we can fetch the grid.cv_results_ to get a glimpse at the results of our pipeline. param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 It seems that we gender of the passenger has more of an effect on their survival than the length of their name. Utility \u00b6 The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code. There's a few small caveats to be aware of. Don't remove data \u00b6 Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this. Don't sort data \u00b6 You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the y variable that you're trying to predict. Don't use lambda \u00b6 There's two ways that you can add a new column to pandas. # Method 1 dataf_new = dataf . copy () # Don't overwrite data! dataf_new [ 'new_column' ] = dataf_new [ 'old_column' ] * 2 # Method 2 dataf_new = dataf . assign ( lambda d : d [ 'old_column' ] * 2 ) In many cases you might argue that method #2 is safer because you do not need to worry about the dataf.copy() that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use lambda functions because it cannot pickle the code. Don't Cheat! \u00b6 The functions that you write are supposed to be stateless in the sense that they don't learn from the data that goes in. You could theoretically bypass this with global variables but by doing so you're doing yourself a disservice. If you do this you'll be cheating the statistics by leaking information.","title":"Human Preprocessing EN"},{"location":"en/guide/function-preprocess/function-preprocessing/#pipe","text":"In pandas there's an amazing trick that you can do with the .pipe method. We'll give a quick overview on how it works but if you're new to this idea you may appreciate this resource or this blogpost . from hulearn.datasets import load_titanic df = load_titanic ( as_frame = True ) X , y = df . drop ( columns = [ 'survived' ]), df [ 'survived' ] X . head ( 4 ) The goal of the titanic dataset is to predict weather or not a passenger survived the disaster. The X variable represents a dataframe with variables that we're going to use to predict survival (stored in y ). Here's a preview of what X might have. pclass name sex age fare sibsp parch 3 Braund, Mr. Owen Harris male 22 7.25 1 0 3 Heikkinen, Miss. Laina female 26 7.925 0 0 3 Allen, Mr. William Henry male 35 8.05 0 0 1 McCarthy, Mr. Timothy J male 54 51.8625 0 0 Let's say we want to do some preprocessing. Maybe the length of name of somebody says something about their status so we'd like to capture that. We could add this feature with this line of code. X [ 'nchar' ] = X [ 'name' ] . str . len () This line of code has downsides though. It changes the original dataset. If we do a lot of this then our code is going to turn into something unmaintainable rather quickly. To prevent this, we might want to change the code into a function. def process ( dataf ): # Make a copy of the dataframe to prevent it from overwriting the original data. dataf = dataf . copy () # Make the changes dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () # Return the name dataframe return dataf We now have a nice function that makes our changes and we can use it like so; X_new = process ( X ) We can do something more powerful though.","title":"Pipe"},{"location":"en/guide/function-preprocess/function-preprocessing/#paramaters","text":"Let's make some more changes to our process function. def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) This function works slightly differently now. The most important part is that the function now accepts arguments that change the way it behaves internally. The function also drops the non-numeric columns at the end. We've changed the way we've defined our function but we're also changing the way that we're going to apply it. # This is equivalent to preprocessing(X) X . pipe ( preprocessing ) The benefit of this notation is that if we have more functions that handle data processing that it would remain a clean overview.","title":"Paramaters"},{"location":"en/guide/function-preprocess/function-preprocessing/#with-pipe","text":"( df . pipe ( set_col_types ) . pipe ( preprocessing , nchar = True , gender = False ) . pipe ( add_time_info ))","title":"With .pipe()"},{"location":"en/guide/function-preprocess/function-preprocessing/#without-pipe","text":"add_time_info ( preprocessing ( set_col_types ( df ), nchar = True , gender = False )) Let's be honest, this looks messy.","title":"Without .pipe()"},{"location":"en/guide/function-preprocess/function-preprocessing/#pipetransformer","text":"It would be great if we could use the preprocessing -function as part of a scikit-learn pipeline that we can benchmark. It'd be great if we could use a function with a pandas .pipe -line in general! For that we've got another feature in our library, the PipeTransformer . from hulearn.preprocessing import PipeTransformer def preprocessing ( dataf , n_char = True , gender = True ): dataf = dataf . copy () if n_char : dataf [ 'nchar' ] = dataf [ 'name' ] . str . len () if gender : dataf [ 'gender' ] = ( dataf [ 'sex' ] == 'male' ) . astype ( \"float\" ) return dataf . drop ( columns = [ \"name\" , \"sex\" ]) # Important, don't forget to declare `n_char` and `gender` here. tfm = PipeTransformer ( preprocessing , n_char = True , gender = True )) The tfm variable now represents a component that can be used in a scikit-learn pipeline. We can also perform a cross-validated benchmark on the parameters our preprocessing function. from sklearn.pipeline import Pipeline from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import GridSearchCV pipe = Pipeline ([ ( 'prep' , tfm ), ( 'mod' , GaussianNB ()) ]) params = { \"prep__n_char\" : [ True , False ], \"prep__gender\" : [ True , False ] } grid = GridSearchCV ( pipe , cv = 3 , param_grid = params ) . fit ( X , y ) Once trained we can fetch the grid.cv_results_ to get a glimpse at the results of our pipeline. param_prep__gender param_prep__n_char mean_test_score True True 0.785714 True False 0.778711 False True 0.70028 False False 0.67507 It seems that we gender of the passenger has more of an effect on their survival than the length of their name.","title":"PipeTransformer"},{"location":"en/guide/function-preprocess/function-preprocessing/#utility","text":"The use-case here has been a relatively simple demonstration on a toy dataset but hopefully you can recognize that this opens up a lot of flexibility for your machine learning pipelines. You can keep the preprocessing interpretable but you can keep everything running by just writing pandas code. There's a few small caveats to be aware of.","title":"Utility"},{"location":"en/guide/function-preprocess/function-preprocessing/#dont-remove-data","text":"Pandas pipelines allow you to filter away rows, scikit-learn on the other hand assumes this does not happen. Please be mindful of this.","title":"Don't remove data"},{"location":"en/guide/function-preprocess/function-preprocessing/#dont-sort-data","text":"You need to keep the order in your dataframe the same because otherwise it will no longer correspond to the y variable that you're trying to predict.","title":"Don't sort data"},{"location":"en/guide/function-preprocess/function-preprocessing/#dont-use-lambda","text":"There's two ways that you can add a new column to pandas. # Method 1 dataf_new = dataf . copy () # Don't overwrite data! dataf_new [ 'new_column' ] = dataf_new [ 'old_column' ] * 2 # Method 2 dataf_new = dataf . assign ( lambda d : d [ 'old_column' ] * 2 ) In many cases you might argue that method #2 is safer because you do not need to worry about the dataf.copy() that needs to happen. In our case however, we cannot use it. The grid-search no longer works inside of scikit-learn if you use lambda functions because it cannot pickle the code.","title":"Don't use lambda"},{"location":"en/guide/function-preprocess/function-preprocessing/#dont-cheat","text":"The functions that you write are supposed to be stateless in the sense that they don't learn from the data that goes in. You could theoretically bypass this with global variables but by doing so you're doing yourself a disservice. If you do this you'll be cheating the statistics by leaking information.","title":"Don't Cheat!"}]}